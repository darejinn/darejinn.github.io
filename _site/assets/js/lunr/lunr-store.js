var store = [{
        "title": "사람다움",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   휘몰아치듯이 많은 생각을 가시화하면, 종잡을 수 없던 방향이 잡혀 나가는 느낌이 좋아서 대학교 1학년 때부터 다이어리에 생각을 기록해왔다. 혼자의 취미를 블로그를 통해 세상과 공유하기로 결심한 이유는 몇년 간의 대학생활 동안 얻은 깨달음 때문이다.   생각은 타인과 나눌 때 성장한다.       우리는 더욱 사람다워질 필요가 있다.   인공지능의 위력이 만연히 커지는 시대이다. 사람의 개입 없이 데이터를 이해하고, 규칙을 찾고, 그 규칙을 새로운 것에 적용할 수 있게 되었다. 계산기와 계산 속도를 비교하는 것이 무의미하듯, 사람과 인공지능은 같은 방향으로 달리는 것이 아니기에 인공지능이 잘하는 영역에서 인공지능보다 앞서고자 노력하는 것은 무의미하다고 생각한다.   인공지능을 배우며, 그렇다면 사람다움이 무엇인지 수십 번 고민했다. 사람은 어떤 존재인가, 다가오는 세상에서 우리는 무엇을 추구해야 하는가. 아래는 내가 생각하는 사람의 특성이다.                      직관 : 한 사람의 무수한 경험이 연결되어 순간의 직관을 만들어낸다. 경험의 산물이지만 학습을 통한 추론과 다른 이유는, 예측할 수 없기 때문이다. 점의 연결을 생각하며 그림을 그리는 점묘법과 달리, 흩뿌렸더니 어느새 나름의 그림이 되어버린 현대미술과 비슷하다.                  호기심과 창발성 : 우리는 때때로 일상 속의 당연한 것에 질문을 던진다. 있는 그대로 받아들이지 않는 사람의 능력이 새로운 패러다임을 만들며 지식의 원을 확장한다.                  감정 : 사람은 감정에 귀기울일 때, 종종 효율에 반하는 무모하고 비논리적인 선택을 한다. 좋아하는 친구와 시덥지 않은 장난을 주고받으며 보내는 몇시간이 아깝지 않고, 사랑하는 누군가를 위해 본인의 소중한 것을 내던져 버리기도 한다. 그러나 정량화도, 일반화도 불가능한 이 ‘감정’이 관여하여 세상의 많은 일들이 이루어진다.                  사람이 대체되는 시대가 아닌, 사람이 더욱 사람다워지는 시대라고 생각한다.   가슴 뛰는 세상이다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/01.-%EC%82%AC%EB%9E%8C%EB%8B%A4%EC%9B%80/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "주식을 대하는 마음가짐",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   나는 어떤 일을 할 때, 가시적인 결과를 생각하는 것을 내켜하지 않는 성격이다.   두 가지 이유가 있는데,     첫째로는 과정을 목적을 위한 수단으로 생각해버려, 과정에서 얻는 만족감과 충만함에 집중하지 못할까 염려스러워서이고   두번째로는 이해타산적인 태도에 대한 무언지 모를 거부감 때문이다.   비슷한 맥락에서 가시적이고 측정 가능한 재화의 대표 격인 ‘돈’은 그리 관심을 갖고 싶지 않은 분야였고, 돈을 다루는 주식 역시 그러했다.   그러나 자의 반 타의 반으로 시작한 주식은 생각보다 더 흥미로웠다.       주식을 대하는 마음가짐      가장 기본적인 원칙은, 내가 정한 원칙대로 매매하는 것이다.    주식에는 공식도, 왕도도 없다고 생각한다. 항상 예측 불가능한 변수가 존재하고, 이 변수를 어디까지 / 어떻게 고려할 것이냐에 따라서 추구하는 매매의 양상이 달라진다.   그러나 중요한 것은, ‘내가 추구하는 매매의 양상’을 알고 원칙을 정하는 일이다. 그렇지 않았을 때, 순간의 감정에 휘둘려 ‘내가 추구하는 매매의 양상’을 잊어버리기 정말 쉬움을 알았다.               절대 조급해하지 않는다. 매매하기 전에, ‘지금의 내가 충분히 이성적인가’를 고민하자.     주식의 상승에 자만하거나, 하락에 마음아파하지 않는다. 주식은 변수를 내재한다. 내가 완벽히 예측해서 상승한 것도, 나의 원칙이 틀려서 하락한 것도 아니다.     현금을 들고 있는 것을 아까워하지 말자(특히 상승장에서 익절한 뒤)     하락하는 주식을 사기 전에, 하락하는 이유가 있는지 살피자. 하락할만 해서 하락한 건, 살 이유가 될 수 없다.     ","categories": ["Finance"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/finance/%EC%A3%BC%EC%8B%9D%EC%9D%84-%EB%8C%80%ED%95%98%EB%8A%94-%EB%A7%88%EC%9D%8C%EA%B0%80%EC%A7%90/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "[KAIST WURF]카이스트 SURF/WURF 소개",
        "excerpt":"  Introduction      2025년 1월-2월, 약 4주 간 진행되는 KAIST 의과학대학원의 동계 인턴십 프로그램인 WURF에 참여한다. 신청하고 알아볼 때 관련 정보가 거의 없어 애를 먹었던 지라, SURF(여름 인턴십) 혹은 WURF(겨울 인턴십)에 관심이 있는 누군가에게 도움이 되길 바라며 작성한다.       1. 신청 배경 / 과정   지원을 결심한 시점이 지원 마감 전날이었던지라 급하게 지원서를 작성해서 지원하였다.   신청 과정은 간단하다.      구글 설문지를 통해 지원서 작성            소개 및 지원 동기       본인의 장점       장래 계획       희망사항(랩실 지망 순서, 배우고 싶은 내용)           약 일주일 안에 선발 결과 메일 통보   경쟁률은 약 5:1이었다고 한다.(총 53명 / 256명)   관련 추가 정보          휴학생은 신청 불가능한가?  홈페이지 안내에는 오직 재학생만 신청이 가능하다고 써져 있다. 본인은 WURF 시점에 휴학 처리된 상황이었고, 지원서에도 휴학중임을 작성하였지만 참여의 기회가 있었으며, 같이 WURF에 참여한 학생들 중에서도 휴학 중인 의과대학 학생이 종종 있었다. 다만, 시기적으로 의과대학 학생들은 거의 대부분 휴학한 상태이기에 상황을 감안해주신 것 같기도 하다. 재학생은 인턴기간 동안 소정의 인턴비를 지급받으나, 절차 상 휴학생은 지급받을 수 없다. 대신, 기숙사 비용을 지원해주셨다.     지내는 곳 / 방배정  캠프 전 메일을 통하여 관련 정보(기숙사 신청 여부 확인, 기숙사 입소 날짜, 출입증, OT 일정 등)를 받았으며, 안내해주시는 선생님께서 굉장히 친절하게 모든 문의에 답해주신다. 카이스트 기숙사를 신청하여 배정받거나, 통학이 가능하다. 기숙사를 신청할 경우 2인 혹은 3인 1실로 WURF를 하는 학생끼리 배정받는다. 룸메이트는 따로 지정할 수는 없으나, 여담으로 친구 A는 신청할 때 구구절절 B와 같은 방을 쓰지 못하면 본인은 잠을 자지 못한다며… 찡찡거려 같은 방으로 배정받았다고 한다.     수료 기준 / 개인 일정  오전 강의를 80% 이상 출석할 시 수료할 수 있으며, 오후 일정은 물론 교수님과 랩실 선생님들께 누가 되지 않는 선에서 양해를 구해야겠지만, 개인적인 일정을 유하게 고려해주신다. 본인도 대전에서 지내면서 수요일 저녁마다 서울에 다녀왔다. 전반적인 프로그램의 목적이, 학생들에게 고강도의 연구인턴을 경험하게 해주겠다기보다, 교수님의 연구들을 가볍게 접하고 학생들 간 네트워킹을 하는 것에 더 비중이 있는 것 같다. 굉장히 여유롭고 편안한 일정이다.          2. 프로그램 소개  프로그램 구성     Meet the professor 오전 강의(진로 탐색) : 매일 카이스트 의과학대학원 교수님게서 교수님의 연구 분야와, 삶에 대한 이야기를 1시간 가량 강의   오후 연구 인턴십(연구활동 참여, 사전에 학생들에게 지망을 받고 배정함)   네트워킹            Tea time(이른 오후 시간) : 1회(공식)       저녁식사 + 뒷풀이 : 2회(공식)           카이스트 의과학 대학원 소개     첫날 의과학대학원 소개 시간에, 구태윤 교수님께서 대학원에 관해 설명해주셨다. 자세한 내용은 입시설명회 혹은 관련 자료에서 확인할 수 있지만, 간단하게만 설명하겠다.     석박사 통합과정   의사과학자 과정 / 의과학자 과정은 전형일 뿐, 교육과정은 정확히 동일함을 강조하셨다.   4월 가을학기 입시, 7월 봄학기 입시로 봄학기 입시가 굉장히 빠른 편이라 미리 준비해야 한다. 온라인 입시 설명회 입시 접수 1개월 전   입학생 534명, 졸업생 300명   재학생 172명 중 의사과학자가 50명, 의과학자가 122명   28명 교수님, 8개 분야, 50명까지 늘릴 계획이다   미래 의과학분야 교원 임용 : 중개의학전문가, 디지털 의생명과학자   문지캠퍼스로 이동(리모델링)   기타 공지(개별인턴십)  첫날 오티를 들으며, 특이하다고 느낀 것이 교수님께서 WURF 기간 이후 기존 연구실에서의 인턴십 연장, 혹은 타 연구실에서의 인턴십을 적극 추천하셨다. 카이스트 의과학대학원은 특히 학부생을 따로 받지 않는지라, 학부생을 귀찮아하지 않는 편이라고, 특히 SURF/WURF를 통해 들어온 학생의 경우에는 대부분 교수님께서 환영하신다고 한다.      meet the professor 시간에 강의하신 교수님의 연구실 중 관심 있는 연구실이 있다면, 메일로 면담신청을 해볼 것   개별 인턴십(연장 혹은 타 연구실) 희망 시 숙소(기숙사 등) 미리 알아볼 것   위 두 가지를 말씀하셨다.  ","categories": ["Projects"],
        "tags": ["일지"],
        "url": "http://localhost:4000/projects/KAIST_WURF_INTRO/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "타인에 관하여",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction : 우리는 모두 다르지만   타인을 아는 것은 불가능하다. 설령 같은 경험을 하고 같은 세상을 보더라도, 우리는 다르게 느끼고 다르게 생각한다. ‘나라면 이렇게 할텐데’하며 실망하지 않고, 내가 감히 알 수 없는 감정의 크기를 함부로 재지 않고, 온전히 존중하는 것은 다른 사람이 나와 다르다는 것을 앎에서 시작한다.   상대를 예단하지 않겠다고 생각했다. 또한 조금 타인을 덜 신경써도 되겠다고 생각했다. 어차피 내가 알 수 없는 영역이니, 최소한의 사회적으로 통용되는 도리를 지키며 그 외적인 것들은 조금 더 마음 편하게 해도 되겠구나 싶었다.   그러나 일련의 경험들을 통해 서로를 온전히 알 수 없다고 하더라도, 알아가려는 노력의 가치가 없어지는 것이 아님을 깨달았다. 타인에 대하여, 함께 사는 세상에 대하여 생각을 정리한다.       함께 사는 세상   주변의 누군가에게 따스함을 느끼는 경험을 많이 하는 요즈음이다. 나의 행복은 나의 안녕에서 비롯하는 줄 알았는데, 누군가의 말 한마디가, 관심과 웃음이, 세심한 배려가, 공유하는 이야기가 가져다주는 행복을 경험하니, 나 또한 이런 따뜻함을 주는 사람이 되고 싶다는 생각이 들었다.   말하기 부담일 수 있는 것들은 먼저 묻지 않는 것,   상대가 공감하기 어려운 이야기를 화두로 올리지 않는 것,   누군가가 사준 음식을 맛있게 먹는 것,   먼저 밝게 인사하고 안부을 묻는 것,   친구의 연락에 답장을 미루지 않고 좋아함을 표현하는 것,   고마운 일에 고맙다고 이야기하는 것,   상대가 ‘갚아야겠다’는 부담을 느끼지 않도록 선을 넘지 않으며 은은하게 배려하는 것.   상대방이 조금 더 행복하지 않을까 고민하고 상상하는 과정이 내게도 행복을 줌을 알았다.   그러나 배려는 ‘상대방은 이럴 거야’라고 생각함으로써 이루어지는, 타자성에 반하는 행위이기도 하다. 그렇기에 아래의 것들을 더욱 주의하고자 한다.   1. 배려의 목적은 상대의 안녕이다.   세심한 누군가에게 내가 좋은 사람으로 보이길 바라며 배려를 행하는 것은, 자기포장에 불과하다. 나를 포장하는 일도 분명 때때로 필요하며, 나를 표현하는 도구가 될 수 있다. 그러나 다른 사람에게 어떻게 보일지를 고려하게 되는 순간, 상대의 안녕보다는 보이는 나의 모습이 더 중요해지며 본래의 가치를 온전하게 느끼지 못하게 되더라.   또한, 나의 배려는 오직 나의 판단에서 비롯했기에, 상대에게는 고마워할 의무도 비슷한 배려를 해야 할 책임도 없다. 오히려 나의 배려가 상대에게 부담이 되었거나, 예상하지 못한 안좋은 영향을 미친 것은 아닌지 살펴야 하는 책임은 나에게 있다. 상대방이 고마워하고 알아주길 바라며 행하는 행동은 나도, 상대도 힘들게 만들 뿐이다.   나의 행동으로 누군가가 안녕해진다면, 그것으로 충분하다. 원래의 목적을 잃은 배려는 상대를 위한 것이 아니다.   2. 모든 것에 앞서서, 나를 잃지 말아야 한다.   나를 잃으면서까지 누군가를 배려한다면, 어쩔 수 없이 내가 내어준 것이 크게 보이기 마련이다.  대가 없이 상대의 안녕을 바랄 수 있는 예쁜 마음은, 내가 줄 수 있는 단단함을 갖추었을 때 생기더라.   다른 사람을 안을 수 있는 품을 갖춘 사람이 되고 싶다.   ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/02.-%ED%83%80%EC%9D%B8%EC%97%90-%EA%B4%80%ED%95%98%EC%97%AC/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "가만히 있지 않는다는 것",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction : 가만히 있으면 반이라도 간다.   가만히 있는 것은 실로 매력적인 선택지이다. 오죽하면 ‘가만히 있으면 반이라도 간다.’, ‘말할까 말까 할 때는 말하지 말라’라는 이야기가 나왔겠는가.   나의 ‘가만히 있지 않음’에 타인과의 상호작용이 관여할 때 고려해야 할 것들은 더 많아진다.   절대적인 선과 악이 없는 모호한 세상에서, 나의 선이 누군가에게는 악이 될 수 있으며 내가 가벼이 여긴 타인의 이야기가 당사자에게는 다른 무게와 깊이일 수 있다.   가만히 있지 않는다는 것은 곧 그 시점의 나의 일부를 타인에게 표현하는 일이기에 누군가는 그 일부로써 나를 정의할 것이다.   그러나 이 많은 머리아픈 일들을 감수하며 말하고 행동할 때, 세상은 움직이고 관계는 변화한다.       반은 가겠지만, 가는 방향을 바꿀 수 없다.   가만히 있음은 멈춤을 의미하지 않는다. 이 잠깐 사이에도 지구가 도는 속도로 돌고 있는 것처럼, 세상의 많은 흐름들에 속하는 우리는 그 흐름대로 끊임없이 가는 중이다.   같은 방향으로 흐르는 주변인들과 관계를 맺고 마음을 나누기에, 집단 속에서 가만히 집단의 방향대로 흐르는 것은 더욱 안정적이다.   그러나 누군가는 이 흐름의 키를 잡고 있겠지, 맞는 방향으로 조정하고 있겠지, 하는 생각이 안일한 안주였음은 보통 무작정 흐르다 도착한 벼랑의 끝에서 드러난다.   집단의 방향을 바꾸는 것에는 세 번의 큰 노력이 필요하다. 당장의 안정감에서 벗어나 ‘이 방향이 맞아?’라는 의구심을 가지는 것에 한 번, 또 이 의구심을 대책과 함께 세심하게 제시하는 것에 한 번, 마지막으로 내가 바꾼 방향이 향하는 곳을 끝까지 살피는 것에 한 번.   모든 과정이 쉽지 않지만, 특히 세번째 과정이 수반하는 책임감이 어마어마하다고 생각하였다. 예측 불가능한 결과를 책임져야 한다는 부담은, 때로는 의구심을 제기하는 대신 아예 집단에 속하지 않는 것을 선택하게 만들었다.   그러나 예측 불가능함은 곧, 생각하지 못한 더 좋은 방향으로 나아갈 가능성을 의미하기도 한다. 건강한 토의가 서로의 불완전한 생각을 보완한다.   소중한 내 주변인이 나의 서툶을 보듬어줄 이들임을 신뢰하고자 한다. 그렇기에 가만히 있지 않고 ‘이 방향이 맞아?’라고 용기내어 말해보고자 한다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/03.-%EA%B0%80%EB%A7%8C%ED%9E%88-%EC%9E%88%EC%A7%80-%EC%95%8A%EB%8A%94%EB%8B%A4%EB%8A%94-%EA%B2%83/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "명분과 실리",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   세상의 많은 일에는, ‘어떻게 보여지는가’가 관여하며   가진 것이 많은 사람일수록 이러한 측정 불가능한 가치가 중요해진다.   좋은 지도자는, 강한 자의 명분을 지켜주며 약한 자의 실리를 추구해야 한다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/04.-%EB%AA%85%EB%B6%84%EA%B3%BC-%EC%8B%A4%EB%A6%AC/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "Solving Challenges in Multi-modal Contrastive Learning",
        "excerpt":"  Introduction  ‘다른 modality’는, 단순히 ‘다른 내용’을 의미하지 않는다. 내용뿐만 아니라 담긴 정보의 수준과 층위가 아예 다르기 때문이다. 가령, ‘예쁜 강아지’라는 글에서의 ‘강아지’가 갖는 해석 가능성이 다양하고 포괄적인 정보와, 강아지 사진에서 ‘강아지’의 확정적인 생김새가 주는 정보를 상상하면 직관적으로 이해가 될 것이다.   따라서, 다른 modality의 정보 사이의 유사성을 비교하는 contrastive learning의 과정에서는 individual modality의 정보가 손실되는 등의, 제대로 align하지 않아 생기는 여러 문제가 존재한다.   이에, CLIP loss가 제안된 이후 multi modal contrastive learning의 문제점들과, 이를 해결하고자 등장한 방법론 3개를 정리하고자 한다. 구체적인 evaluation 과정보다는, 논문의 아이디어를 중심으로 전개하겠다.         Remaining Challenges   1. Heterogeneity Gap     두 모달리티를 align하는 데에 있어서,  modality의 이질성(heterogenity gap)을 고려해야 한다. 각 모달리티 이질성을 고려하지 않고 형식적인 align을 수행하면, 모델이 일부 중요한 정보를 놓치거나(underalignment), 반대로 중복되거나 무관한 정보를 혼동하게(overalignment) 만들 수 있다. 결과적으로 멀티모달 대조 학습의 근간인 양 modalities 간 semantic support가 약화되어, 최종 표현 학습의 효율이 떨어지게 된다. 두 가지 heterogenity gap을 생각해볼 수 있다.   1.1. 추상화 레벨의 불일치(granuality mismatch)   멀티모달 데이터에서 서로 다른 modality는 서로 다른 의미/표현 수준의 정보를 전달한다.   이미지와 텍스트의 추상화 레벨 불일치   예컨대 오른쪽 강아지 이미지의 경우, 해당 이미지는 “웃고 있는 개”라는 상위 개념뿐만 아니라 품종, 털 색상, 크기, 형태 등 다양한 하위 수준의 속성을 포함하고 있다. 반면, “잔디밭에서 웃고 있는 개”라는 텍스트 설명은 일반적으로 더 추상적이고 압축된 정보만 담고 있다. 이러한 불일치는 멀티모달 대조 학습 과정에서 서로 다른 수준의 의미를 동등하게 맵핑하기 어렵게 만든다. 특히 시각적으로 표현된 하위 속성(예: 털의 질감, 표정 등)이 텍스트 캡션에서 직접 언급되지 않으면, 대응되는 의미를 찾지 못해 학습이 불안정해질 수 있다.   1.2. 구조적 이질성   이미지와 텍스트의 처리 방식  이미지를 작은 패치 단위로 나누어 특징을 추출하는 방식과, 텍스트를 토큰(단어·문장 등) 단위로 나누는 방식 간에는 불가피한 구조적 차이가 존재한다.      합성곱 신경망(CNN)이나 비전 트랜스포머(ViT)를 통해 처리되는 패치는 픽셀 값의 2D 그리드로 구성되어 있으며, 공간적 관계와 질감 정보를 포함한다.   텍스트에서 BERT나 GPT와 같은 언어 모델로 처리되는 순차적인 토큰은, 문법적 구조와 의미론적 관계를 포함한다.   CLIP(Contrastive Language-Image Pre-training)과 같은 초기 모델들은 이미지와 텍스트를 매칭하여 관계를 학습하나, 이러한 모델들은 시각적 패치와 텍스트 토큰을 집계하여 표현할 뿐, 동일한 세밀도 수준에서 시각적 및 의미적 개념을 명시적으로 정렬하지 않는다. 이를 단순 병렬로 처리하거나 동일 차원으로 맞추는 것만으로는 세밀한 의미 정렬이 보장되지 않는다.   2. 결측 모달리티(Missing Modality) 문제     멀티모달 모델을 응용하는 실제 상황에서는 이미지 또는 텍스트가 일부 누락된 상태로 데이터를 받는 경우가 빈번하게 발생한다. ‘다양한 모달리티’를 복합적으로 고려하여 풍부하고 정확한 해석을 하고자 만든 모델인데, 오히려 모달리티의 다양성이 제약조건을 만들어버리는 것이다. 결측 상황은 단순한 예외적 상황으로 치부해버리기엔 너무나 보편적인 문제이기에, 멀티모달 시스템이 필수적으로 해결해야 하는 과제로 여겨지고 있다.   가령, 아래와 같은 상황이 있을 수 있겠다.      이미지는 있지만 그에 대한 텍스트 태그나 캡션이 없는 경우   멀티모달 센서 네트워크에서 특정 센서의 일시적 장애로 인한 데이터 결측   의료 진단 시스템에서 특정 검사 결과의 부재   학습 시에는 모든 모달리티 정보를 이용할 수 있으나, 추론 과정에서는 특정 모달리티가 결측되는 현상(train-inference discrepancy)은 모델의 일반화 성능을 크게 저하시킬 수 있다.   “[CVPR 2020] Gradient-Blending: Learning Modalities with Varying Rates”의 내용을 바탕으로 살펴보면, 학습된 모델은 특정 모달리티(특히 많은 정보가 있는 모달리티)에 의존하는 편향을 보일 수 있으며, 이러한 편향은 해당 모달리티가 결측되었을 때 성능이 급격히 저하되는 현상으로 이어진다. 해당 논문에서는 일부 멀티모달 모델은 단일 모달리티 입력 시 성능이 무작위 추측 수준으로 떨어지는 현상을 관찰하였다.         Solving Challenges in Multi-Modal Contrastive Learning   모달리티별 이질적인 표현을 공유 reprentation space에 정렬하고, missing modality의 문제를 다루는 시도들은 꾸준히 이루어지고 있다. 가령 각 모달리티의 확률 분포를 추정하는 VAE 기반 방법, 이산 표현(코드북)을 사용하는 벡터 양자화(VQ) 기반 접근법 등을 들 수 있겠다. 본 글에서는, contrastive learning 으로 학습하는 모델 구조를 큰 틀에서 변경하지 않고, 위 문제들을 해결하고자 시도한 논문 세 편의 방법론을 정리한다. 세 논문 모두 공통의 latent space에서의 representation align 을 목표하며, 세번째 논문은 missing modality의 문제를 함께 다룬다.   1. Unified Multi-modal Training (Intra- &amp; Inter-modal Similarity Preservation)        [ICML 2022] Multimodal Contrastive Training for Visual Representation Learning     💡아이디어 : 통합 학습 프레임워크(Unified Training Framework):      Intra-modal Training Path를 통해 각 모달리티 내에서 data augmentation에 의한 self-supervised 학습을 수행하며, intrinsic data properties를 최대한 보존한다.   Inter-modal Training Scheme를 통해 이미지와 텍스트 등 서로 다른 모달리티 간의 cross-modal interactions를 강화하여, 공통 semantic space 내에서 유사도를 보존하도록 학습한다.   1.1. Objective   본 논문에서는, 이미지와 텍스트 데이터를 바탕으로, visual representation을 학습하는 것을 목표한다. 논문에서 강조하는 점은 cross-modal correlation을 배우는 것을 넘어서서 각 modality의 intrinsic data property를 unified framework로 최대한 끌어낸다는 것이다. (저자들은 similarity preservation이라고 표현한다.)             (d)가 논문의 방법으로, 2번/3번과 같은 modality 안 학습과, 4번/5번과 같은 modality 사이 학습을 동시에 진행한다.    1.2. Method            위 그림에서, 주황색과 초록색이 modality 내 학습을, 노란색과 초록색이 modality 간 학습을 의미하며, 각각 다른 constrasive loss를 사용한다.            Modality 내 학습 : MoCo-v2 framework       본 논문에서는, modality 내의 unsupervised visual representation learning을 위해 Momentum Contrast(MoCo)라는 방법을 차용한다.              Momentum Contrast(MoCo) Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning.         MOCO         이미지는 각 픽셀이 연관되어 있고, 고차원이기에 tokenized word dictionary와 같이 구조화된 dictionary를 만들 수 없다. 따라서 dynamic dictionary (동적사전)가 필요한데, MoCo는 이 사전을 &lt;크고, 안정적으로&gt; 만드는 방법으로 제안되었다.                 key는 데이터(이미지, patch 등)에서 sampling을 한 후 momentum encoder를 통해 표현이 된다.                        ‘momentum encoder’이라고 이름붙여진 이유는 다음과 같다.  key를 만들어내는 encoder가 빠르게 학습이 되면 representation이 빠르게 바뀌기 때문에 이전에 dictionary의 key들이 다 소용이 없어지게 된다. 그렇기 때문에 momentum을 이용해 조금씩 변화를 주어서 한번에 큰 변경이 없게 만들어 학습을 안정적으로 진행한다.                             query encoder는 momentum encoder과 달리 적극적으로 학습된다. query는 matching 되는 key와 가깝고, 다른 key와는 다르게 constrasive learning이 이루어진다.                  저자들은 기존 MoCo에서 text encoder/text momentum encoder를 추가로 도입하고, tag information을 loss에 추가하여 high-level concept의 pattern도 학습하게 하였다.            Modality 간 학습 : common space mapping + contrastive learning       크게 특별할 것 없이, 이미지(CNN 기반)와 캡션(Bert-like transformer 기반)을 각각 독립적인 MLP projection head를 통해 공통 공간으로 매핑한 뒤, 양방향 contrastive loss를 설계한다.              Image-to-Caption: 이미지 표현과 대응하는 캡션 표현의 유사도를 높이고, 나머지 음성 샘플들과의 유사도는 낮춤.       Caption-to-Image: 캡션 표현과 대응하는 이미지 표현의 유사도를 높이고, 나머지는 낮춤.           2. Finite Discrete Tokens (FDT)        [CVPR2023] Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens    💡아이디어 : Finite Discrete Tokens (FDT):      학습 가능한 일정 수의 discrete tokens를 사전의 단어들마냥 도입하고, 이미지와 텍스트 모두를 동일한 FDT 집합의 sparse attention-based aggregation으로 표현한다.   기존 [이미지 패치의 가중합]과 [단어 토큰의 가중합]의 유사도 비교 대신 [FDT의 가중합 1]과 [FDT의 가중합 2]의 유사도 비교를 함으로써 1) FDT라는 같은 granuality에서의 비교 2) 두 모달리티의 진정한 의미론적 비교 를 가능하게 한다.   2.1. Objective   오른쪽이 논문의 방법이다.   저자들은, CLIP 기반 모델에서, cross-modal information을 두개의 독립적 인코더로 인코딩한 뒤 바로 similarity를 비교하는 방식의 한계를 지적한다. 두 representation의 granualities(세밀한 정도)가 다르기 때문이다. 저자들은 같은 level의 granuarity를 가지는 정보 간 contrasive learning을 수행하는 것을 목표한다. 이를 위하여 본 논문은 두 information의 정보들을 각각 FDT라는 공통된 토큰집합을 매개로 표현한 뒤 학습을 수행한다.   2.2. Method   전체 framework(왼), FDT based Feature generation 방법(오)   FDT를 이용해서 두 modality의 granuality를 맞추어 contrasive learning을 수행하는 방법은 위 모식과 같다. FDT(Finite Discrete Tokens)는, 왼쪽 가운데에 표현된 노란색 토큰들의 집합이다. 이 토큰들을, 사전의 단어들이라고 생각할 수 있다. Image의 FDT based Feature를 구하는 것은, 결국 이 동적 사전의 단어들의 가중합으로 이미지를 표현하는 것을 의미한다. 방식은 아래와 같다.   a. 먼저, 패치와 FDT 간의 내적 계산으로 N개의 패치가 FDT들과 얼마나 유사한지 나타내는 attention matrix가 구해진다.(오른쪽 가운데 회색 matrix) 이때 각 패치를 query, FDT를 key라고 볼 수 있겠다.   b. Max pooling을 통하여, C개의 FDT 각각이 이미지와 얼마나 유사한지의 유사도를 구한다.   c. FDT의 토큰들을 앞서 구한 유사도를 가중합하여 최종적으로 FDT based Feature를 구한다.   이렇게 구해진 FDT based image Feature, FDT based text Feature간 contrasive learning을 수행한다.   FDT는 각 이미지 패치와, 텍스트 토큰이 의미를 알려주는 prior knowledge의 기능을 수행한다. 이때 유의해야 할 것은, 결국 학습의 궁극적 목적은 text encoder, image encoder가 각각 유의미한 representation을 인코딩하게 학습되는 것이며 FDT는 이를 도와주는 역할이라는 것이다. Text FDT grounding, image FDT grounding 자체가 나중에 쓰이는 건 아니다.   3. Geometric Multimodal Contrastive (GMC)        [ICML 2022] Geometric Multimodal Contrastive Representation Learning    💡 아이디어 : Geometric Multimodal Contrastive Loss      전체 모달리티가 존재하는 complete observation과, 결측(modality missing)된 상황의 representation을 서로 가까이 정렬하도록 기하학적으로 학습하는 novel한 loss를 설계하였다.   3.1. Objective   가운데의 Z1:2가 complete modality representation이다.   해당 논문은, modality의 종류나 개수를 한정짓지 않는 새로운 프레임워크를 제안한다. 저자들은 1) modality 간 hetrogenity gap 2) missing modality 문제를 동시에 해결하는 것을 목표한다.   이를 위하여 모달리티 특이적 encoding만을 진행하는 기존 방법과 달리 모든 모달리티가 통합된 complete modality representation을  추가로 도입하여, 공통 공간에서 align한다.   3.2. Method   전체 framework   a. Two-level Architecture     - Modality-specific Base Encoders를 이용해 각 모달리티와 complete-modality를 고정 차원의 intermediate representation으로 변환한다. (위 그림의 f)     - 이후, Shared Projection Head를 거쳐 모든 모달리티를 공통의 latent representation space로 매핑한다. (위 그림의 g)   b. Geometric Multimodal Contrastive Learning      contrastive learning은, 위 그림의 공통된 임베딩공간(점선 박스)에서 수행된다.   각 modality의 표현 $z_m$이 동일한 샘플의 complete representation인 $z_{1:M}$과는 가깝도록(위 그림의 빨강/파랑 실선), 다른 샘플의 표현들과는 멀어지도록(위 그림의 점선) 학습한다.   이러한 geometric alignment는, 각 modality가 스스로 complete representation과 의미관계가 가까워지도록 학습하기 때문에, 결측 modality가 존재해도 남은 modality만으로도 충분히 전체 의미를 유추할 수 있는 표현을 생성할 수 있게 된다.   또한, modality의 fusion이 강제되는 기존 방식과 달리, 각 modality가 독립적으로 학습되어 전체 의미공간으로 연결되므로, modality의 종류/개수가 자유롭다는 점도 위 두 논문과의 차이점이다.         To sum up…   본 글에서는 멀티모달 모델에서 modality 간 이질성(heterogeneity gap)과 결측(modality missing) 문제에 대응하고자 제안된 세 가지 방법론을 살펴보았다.      첫 번째로 소개한 Unified Multimodal Training은 intra-modal과 inter-modal 학습 경로를 동시에 학습하는 통합 프레임워크를 통해, modality별 intrinsic property를 보존하면서도 공통 표현 공간에서의 정렬을 유도한다.   두 번째 방법인 Finite Discrete Tokens (FDT)는 이미지와 텍스트의 표현 granularity 차이를 해결하고자, 양 modality 모두를 학습 가능한 고정된 토큰 집합(FDT)으로 표현하여 더 정밀한 의미 수준에서의 정렬을 가능하게 한다.   마지막으로 Geometric Multimodal Contrastive (GMC)는 complete modality representation과 partial modality representation 간의 기하학적 정렬을 통해, modality 수나 조합, 결측상황에 구애받지 않는 유연성을 제공한다.   이들 방법론은 모두 기존 contrastive learning의 개념을 유지하면서도, align하는 대상과 방법울 다르게 하여 견고한 멀티모달 representation learning을 목표하였다는 점에서 특징적이다.  앞으로의 멀티모달모델이 더욱 다양한 입력 조건과 복잡한 의미 관계를 다루게 될 것을 고려할 때, 이와 같은 새로운 방법론은 더욱 중요한 연구 방향이 될 것으로 기대한다.   정리  ","categories": ["Papers"],
        "tags": ["Multimodal AI","Contrastive Learning"],
        "url": "http://localhost:4000/papers/Solving-Challenges-in-Multi-modal-contrastive-learning/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "Contrarian을 응원하며",
        "excerpt":"  ‘가만히 있지 않는다는 것’글에 대한 I의 답글입니다.   1.     동물의 제 1 목표는 생존임.   인간은 체력, 방어력, 감각능력 등 신체 능력은 다른 개체에 비해 열등한 동물임.   인간은 다른 개체 대비 우월한 사회적 협력과 조직화 능력을 보유하고 있음.   이로 인해 인간은 불리한 신체 조건에도 불구하고 생태계를 지배하는 유일한 종이 되었음.   즉, 인간에게 조직은 개인보다 목표 달성을 위한 우월한 구조임.   2.     현대 사회에서 생존이라는 본능은 다양한 사회적 목표로 분화됨.   좋은 조직은 나의 목표와 조직의 목표의 방향이 일치하는 조직임.   이 경우, 조직을 위하는 것이 곧 나를 위하는 것이 됨.   그렇지 못한 조직은 개인과 조직에게 모두 들어가지 않는 것이 최선, 빨리 나오는 것이 차선임.   3.     관성은 운동 상태의 변화를 거부하는 저항의 성질임.   조직이 방향을 갖고 움직이기 시작하면 해당 방향을 유지하려는 관성이 형성됨.   따라서 조직의 방향을 바꾸려면 관성이라는 저항을 이겨내야 함.   4.     관성을 줄이는 방법은 질량을 줄이는 것임.   즉, 기존의 방향에 대한 판단을 유보하거나, 옳다고 믿던 구성원의 수를 줄여야 함.   따라서 그들에 대한 설득의 과정이 필요할 것임.   5.     방향 전환에 대한 설득을 위해서는 조직의 목표가 무엇인지 재정립이 필요함.   그리고 방향 전환이 조직의 목표를 달성할 수 있는 더 우월한 방법임을 설명할 수 있어야 함.   즉, 목표 달성 차원에서 기존 방향이 열등한 이유를 설명할 수 있거나, 더 나은 대안을 제시할 수 있어야 함.   위 2가지가 이루어지지 않은 반박은 무의미하고 해로움.   6.     조직에게 무의미하고 해로운 영향은 조직의 목표 달성 확률을 떨어뜨림.   이는, 나의 목표 달성 확률을 낮추는 일임.   그러나, 위 2가지가 이루어진 반대의 의견은 조직의 목표 달성 확률을 매우 높임.   이는, 나의 목표 달성 확률을 높이는 일임.   7.     나는 이루고 싶은 목표가 있는가? 나는 조직의 필요성을 인지하고 있는가? 나는 좋은 조직에 속해 있는가?   그렇다면, 내가 속한 조직은 위와 같은 선순환이 가능한 조직일 것임.   그렇다면, 내가 속한 조직은 Contrarian이 필요하며, 준비된 반박은 조직과 나를 위하는 일임.   그렇지 않다면, 가만히 있는 것이 반이라도 갈 수 있는 방법임.   그러나, 조직의 방향에 동의가 되지 않음에도 가만히 있어야 반을 갈 수 있는 조직에서 목표를 이루는 것은 불가능함.  ","categories": ["Bridges"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/bridges/Contrarian%EC%9D%84-%EC%9D%91%EC%9B%90%ED%95%98%EB%A9%B0/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "희소함의 역할",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   사람은 익숙하지 않은 것에서 흥미를 느낀다.   따라서 타인이 나를 주목하게 만드는 일은 그리 어렵지 않다. 다수와 다른 무언가를 하면 된다.           모두가 말할 때 말을 하지 않는 것, 그리고 아무도 말하지 못할 때 목소리를 내는 것.            중요하지 않아 보이는 일에 사력을 다하는 것, 그리고 모두가 목매는 일을 가벼이 넘겨버리는 것.            정해진 출근 시간보다 항상 일찍 출근하는 것. 발표에서 아무도 갖지 않은 의문을 유려히 제시하는 것. 확고한 입맛과 취향이 있는 것.       희소함은 ‘나’라는 존재를 타인에게 어필하는 좋은 도구이다. 이 새로움에 일관적으로 나의 색이 드러날 때, ‘나’는 ‘관종’이 아닌 ‘특별한 사람’으로 인지된다. 타인이 나에게 귀를 기울인다.   내가 타인과 다름을 인지할 때, 나의 특별함이 주는 자기만족의 크기는 꽤나 크다. 또한 일반적으로 타인이 하지 않는 일은 하기 어려운 일이기에 희소함은 성취감을 불러일으키기도 한다.   그러나, 희소함이 내 행동의 내적 동기가 될 수 있는가?       Rationale      Rationale : a set of reasons or a logical basis for a course of action or a particular belief.    좋은 논문의 서론에서 저자는 ‘이 연구가 왜 novel한지’와 더불어, ‘이 연구가 왜 합당한지’를 이야기한다.   아무도 해보지 않았음은 곧, 검증되지 않았음을 의미하기도 한다. 특히 published된 연구가 없음은, ‘아무도 해보지 않았음’이 아니라 ‘해본 모두가 실패했음’의 가능성도 내재한다.   나의 시도가 기존의 시도들에 비해 뚜렷하게 갖는 이점, 혹은 이점을 기대할만한 논리적 근거가 존재하여 스스로가 설득되었을 때 연구자는 확신을 갖고 어떤 어려움도 밀고 나갈 수 있다. 예상과 다른 결과를 마주했을 때 동력을 잃지 않고 문제를 분석할 수 있다.   실패 가능성이 높은 새로운 연구일수록, rationale을 찾는 시도는 불가결하다.       희소함의 역할   연구와 삶의 다른 점은, 타인과의 감정적 상호작용이 관여한다는 것이다. 이는 희소함에 대하여 아래와 같은 고민거리를 더한다.           첫 번째로, 내가 제시한 새로운 가설이 검증의 과정을 거치기도 전에 타인에게 관철될 수 있다. 제안하는 사람의 사람됨, 표현 방식, 명성, 지위 등이 주는 인상은, 비판적 검토 없이 ‘어련히 저 사람의 주장이 맞겠지. 내가 모르는 무언가가 있겠지.’라는 생각을 하게 만든다. 신뢰받는 사람일수록, 집단의 다수는 그 사람이 던진 낯선 주장을 무서울 정도로 수용적으로 받아들인다. 영향력 있는 사람의 근거 없는 주장은 그래서 더 위험하다.            두 번째로, 다름은 빠르게 관철될 수 있는 만큼, 역설적으로 너무 쉽게 틀림으로 변질한다. ‘잘은 모르겠지만 맞겠거니’의 방식으로 쌓아올린 믿음은 아슬아슬하게 쌓은 높은 탑과 같아 약간의 균열만으로 쉽게 무너진다. 함께 사는 세상에서 타인에게 인정받지 못함이 주는 아픔은 크다. 더욱이 나의 희소한 행동이 타인을 의식하여 만들어졌다면, 집단에서 인정받지 못하는 소외감과 외로움은 나의 희소함을 던져버리고 다수 속에 들어가고자 하는 회귀 본능을 불러 일으킨다. 타인으로부터 정의된 동력은 타인이 달라질 때 너무 쉽게 그 의미를 상실한다.       희소함은 더욱 눈에 띄고 매력적으로 나를 포장하지만, 포장에 관계 없이 그 속을 들여다보는 일은 언젠가 이루어진다. 그 과정에서 타인에게 흔들리지 않는 단단한 동력은 포장이 아닌 무게가 만든다.   삶의 순간들에서도, 나의 rationale을 찾고자 한다.   희소함이라는 도구는 사용하는 것이지, 휘둘리는 것이 아니다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/05.-%ED%9D%AC%EC%86%8C%ED%95%A8%EC%9D%98-%EC%97%AD%ED%95%A0-copy/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "재미와 사랑이 양립할 때",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.  Introduction      ‘재미’ : 놀라움을 수반한 즐거움     ‘사랑’ : 타인을 돕고 이해하고 가까이하려는 마음 ; 에로스적 사랑이 아닌, 아가페적 사랑에 초점을 맞춘다.    두 마음은 삶의 많은 일들의 강력하고 근본적인 원동력이다.   재미있는 일에 몰입하고, 사랑하는 누군가의 행복을 기대하며, 우리는 시간이라는 무거운 가치를 아깝지 않게 지불한다.   주로 재미는 자신을 위하는 마음에서, 사랑은 타인을 위하는 마음에서 비롯하기에   때때로 재미의 극단에서 타인을 잃기도 하고,   사랑의 극단에서 자신을 잃기도 한다.       재미와 사랑이 양립할 때   ‘함께’라는 이름으로 재미와 사랑은 양립할 수 있다.   나를 위한 마음과 타인을 위한 마음이 ‘우리를 위한 마음’으로 일원화될 때 만들어지는 시너지는 엄청나다.   공유하는 이해는 재미와 사랑을 동시에 공고히 하며 신뢰를 만든다.   ‘함께’가 가져다주는 안정적인 행복은 연인과, 친구와, 동료라는 원초적인 결속으로 인간을 묶었으리라.       나의 재미는 앎에서 기인한다.   같은 방향을 보는 사람들과 함께하며, 앎의 외연을 확장하는 경험을 하는 요즈음이다.   똑똑한 타인에게서 얻는 새로운 시각이 재미있다. 동시에, 나의 깊이가 함께하는 사람의 재미에 기여함을 느낀다.   나의 치열한 고민은, 함께를 이야기하는 순간부터 타인을 위하는 일이 되기도 하는구나.   재미와 사랑이 양립하는 시간들로 한정적인 삶의 순간들을 한껏 채우고 싶다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/06.-%EC%9E%AC%EB%AF%B8%EC%99%80-%EC%82%AC%EB%9E%91%EC%9D%B4-%EC%96%91%EB%A6%BD%ED%95%A0-%EB%95%8C/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "[Research] Game-based Shadow-induced Forgetting",
        "excerpt":"  This project was completed as Team Soothing.   Game-based Shadow-induced Forgetting - Effects of Consciousness, Emotional Valence, and Temporal Dynamics on Memory Suppression   Preprint Link  Abstract  Background:  Intentional suppression of specific memories disrupts hippocampal function, leading to amnesia of unrelated memories within a brief temporal window—a phenomenon termed the “amnesic shadow”. However, the mechanisms underlying shadow-induced forgetting (ShIF) and its modulation by conscious versus unconscious processing and emotional valence remain unclear.   Methods:  This study examined variations in ShIF by manipulating image exposure conditions (conscious vs. unconscious) and emotional valence (positive vs. negative) across short- and long-term retention periods. A novel game-based approach was developed, where participants’ habitual motor responses were disrupted via reversed game controls, thereby inducing an amnesic shadow. Electroencephalography (EEG) was used to assess neural activity related to memory suppression.   Findings:  Short-term ShIF was significant for consciously cued negative images, as evidenced by reduced memory performance immediately after the intervention. EEG analyses revealed increased inhibitory neural activity during the suppression of negative memories. Over time, the ShIF effect for consciously cued images diminished, whereas a delayed effect emerged for unconsciously cued images, suggesting distinct neural mechanisms underlying conscious versus unconscious forgetting.   Interpretation:  These findings indicate that ShIF operates more effectively for negative and consciously accessed memories, highlighting the role of inhibitory control in memory suppression. Furthermore, the results underscore the potential of game-based interventions in facilitating intentional forgetting. Understanding ShIF mechanisms could contribute to therapeutic applications, particularly in conditions like posttraumatic stress disorder (PTSD) and obsessive-compulsive disorder (OCD).   Keywords:  shadow-induced forgetting, game-based treatment, intentional suppression, electroencephalography, hippocampus  ","categories": ["Projects"],
        "tags": ["Cognition"],
        "url": "http://localhost:4000/projects/Soothing/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "[Competition] Multi-Omics-Based Drug Sensitivity Estimation ",
        "excerpt":"  This project was completed as Team MOUM, in collaboration with my colleagues for the 6th YAI-CON.   💊 Multi-Omics-Based Drug Sensitivity Estimation  6th YAICON — Spring 2025 · Second Prize Code Link     📌 Overview  Accurately predicting how a cancer cell line responds to a drug (IC-50) remains an open challenge: the outcome depends not only on the drug’s chemistry but also on the cell’s intricate molecular profile.  We present an end-to-end deep-learning pipeline that fuses three omics layers (GEP, MUT, CNV) with advanced drug-embedding models (ChemBERTa &amp; graph-based GNN) and a bi-directional cross-attention mechanism. Our approach improves upon the 2025 paper “Anticancer drug response prediction integrating multi-omics pathway-based difference features and multiple deep-learning techniques.”     🌱 Why we built this                  Baseline limitation       Our upgrade                       Drug representation lacks structural cues (only SMILES RNN)       Two interchangeable drug encoders • ChemBERTa — language-style SMILES embedding • BGD — graph transformer on molecular graphs                 Shallow “context attention” can’t model complex drug-omics interplay       Deep, bi-directional cross-attention (drug ↔ each omics) giving 6 interaction maps             🔬 Data                  Source       Entities       Notes                       CCLE       688 cell lines       GEP (log₂ TPM + 1), MUT (0/1/2), CNV (log₂ discrete)                 GDSC2       233 drugs       Matched IC-50 ground-truth                 MSigDB – 619 KEGG pathways       –       Used to derive pathway-difference statistics (Mann-Whitney U / χ²-G)             🛠 Methodology          Omics pathway features  For every cell line × pathway, compute statistical separation between “in-pathway” and “out-pathway” genes → 3 feature matrices of size 1 × 619 (GEP, MUT, CNV).            Drug embeddings  Choose one encoder at training time                                  Encoder           Key idea           Output shape                                           ChemBERTa           Tokenise SMILES, pad to 256, take final hidden CLS           1 × 384                             BGD           Graph transformer over atoms/bonds + DeepChem node feats           1 × 256                                Cross-attention block  • Drug (Q) ↔ Omics (K,V) for each omics type, two directions (drug→omics, omics→drug) → 6 attention layers in total.  • Concatenate pooled outputs → stacked MLP → IC-50 regression.            Implementation diagram above: original (left) vs. modified cross-attention (right).      📂 Code &amp; Repos                  Repository       Description                       Drug-Sensitivity-Prediction-Pipeline       Main training pipeline, model zoo, experiment scripts                 DGL-Life-sci       Custom extensions for graph-based drug encoders            Model zoo snapshots  1. ChemBERTa Drug Embedding         2. Graph-Transformer Drug Embedding             📊 Results          Figure 1. Drug embedding comparison (Original vs. Modified attention)           Figure 2. Cross-attention variant performance           Figure 3. Pearson r on cell-blinded split (scatter)       Key takeaway : Improvement over the baseline when switching to ChemBERTa/BGD-Model + cross-attention. Full metrics in /results/.      👥 Contributors &amp; Acknowledgments   With gratitude to @yumin-c, I’m distilling this work into a focused, application-driven research project in collaboration with several colleagues, including  my original MOUM team members: @bgduck33 and @whdsbwn.   You can find the full names of all MOUM team members on GitHub: Code Link.    ","categories": ["Projects"],
        "tags": ["Multi-Omics"],
        "url": "http://localhost:4000/projects/MOUM/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "Oveview on Graph Representation Learning",
        "excerpt":"  Introduction           Graph(왼쪽)과 Euclidean structured data(오른쪽) (출처: CS224w)   이미지, 문장과 같이 2D grid나 1D sequence로 성분의 순서와 구조가 정형화된 euclidean structured data와 달리, 세상의 많은 요소들은 요소 사이의 순서와 arrangement rule이 고정되어 있지 않은 non-euclidean space에 존재한다. 이러한 non-euclidean space에서는, 기존의 traditional deep learning method들이 그대로 적용되기 어렵다. 단적인 예로, CNN의 convolution filter은, 특정 성분을 중심으로 n*n의 격자 kernel을 정의하지만, 상하좌우, 앞뒤 관계가 분명하지 않은 데이터, 가령 사람과 사람 사이의 네트워크 정보 등에서는, 같은 kernel을 정의하는 것이 불명확해진다.   이러한 non-euclidean space 상의 요소들은, ‘Graph Struture’을 정의함으로써 다룰 수 있다. Node 와 node를 연결하는 edge로 정의되는 그래프는 node 사이의 관계가 euclidian space에 한정되지 않고 자유도가 높다는 장점과 더불어, node feature과 edge weight으로 자유롭게 요소의 정보를 추가할 수 있다는 점에서 매력적이다.   하지만 데이터의 자유도가 큰만큼, graph를 분석하는 방법론 역시 다양하고 방대하다.   그래프를 공부하며, GNN, random walk, message passing, subgraph sampling, graph kernel, graph signal processing, graph structure learning, graph embedding, graph mining 등 끝없이 등장하는 용어들이 어느 층위에서 어떻게 다른 용어들과 연결되는 것인지 명확히 정리되지 않아 혼란함을 느꼈다. 나의 이해를 가시화해나가는 과정을 통해서 더 깊은 이해가 가능해지길, 또 글이 나와 비슷한 고민을 하는 누군가에게 조금이라도 도움이 되길 기대한다.   본 글에서는, graph representation의 framework를 개괄한다.        Graph Representation Learning이 무엇인가    Graph의 용어들  Graph는 정보 unit(nodes)와, 사이의 연결관계(edge)로 이루어진, 나이브한 구조의 정보이다.   2차원으로 flattening된 이미지나, 해석 가능한 단어들의 조합으로 정렬된 언어와 달리, graph stucture의 high dimensional, non-euclidian information은 직관적인 해석이 불가하다. 따라서, 그래프에서 필요에 맞는 유용한 정보를 extract하는 방법은 오래전부터, 많은 분야에서 다루어져왔다.          Graph (출처: CS224w)    일반적으로 그래프 $G$는 $G = (V, E)$의 tuple로 표현된다.      이때, $V = {v_1, v_2, \\ldots, v_n}$ 는 $n$개의 노드 집합을 의미한다.   $E = {e_1, e_2, \\ldots, e_m} \\subseteq V \\times V$ 는 $m$개의 edge (노드와 노드를 연결하는 링크)를 의미한다.   각 노드는 특성을 가질 수 있으며 (예: 노드가 사람일 경우, 성별이라는 특성이 각 노드에 정의될 수 있다) $k$ 종류의 특성이 있을 경우, 노드 특성 행렬  \\(\\mathbf{X} \\in \\mathbb{R}^{k \\times |V|}\\) 로 표현된다.   이렇게 정의된 그래프에서 edge의 양상은 인접행렬 (Adjacency Matrix)  \\(\\mathbf{A} \\in \\mathbb{R}^{|V| \\times |V|}\\) 로 표현되며 만약 $(v_i, v_j) \\in E$이면 $A_{ij} = 1$, 그렇지 않으면 $A_{ij} = 0$이다.    Learning의 목표는 그래프의 구조적 정보를 보존하는 것이다.   그래프를 활용하기 위해서는, 그래프와 그 구성 요소들(예: 노드 및 엣지)을 다룰 수 있는 numerical features로 표현할 필요가 있다. 물론 위에 설명한 인접 행렬 $\\mathbf{A}$ 자체도 그래프를 표현하는 방법이다. 그러나 인접 행렬은 크기가 $|V| \\times |V|$이므로 매우 큰 그래프를 표현하기에는 소모적이며, 그래프의 중요한 정보를 보존하되 차원을 줄이는 feature extraction이 필요하다.          Graph Representation Learning (출처: CS224w)      고전적인 방법들은, 그래프에서 사전에 정의된 알고리즘으로 그래프에서 정보를 extract한다. Graph statistics(degrees, clustering coefficients), kernel functions 등이 이에 해당한다.  이러한 hand-engineered feature들은 산출 방법이 정해져 있는 ‘processing’이며, inflexible하다. 이러한 방법들은, 어느 정도 graph property를 반영하긴 하나, 오직 그래프를 정해진 방법으로 가공한 것이므로 feature의 깊이에 한계가 있다.   Graph Represenation Learning은, 이와 달리 그래프의 구조적 정보를 잘 보존하도록 embedding을 ‘learning’한다. ‘learning’한다는 것은, 설정된 ‘goal’에 가까워지게 최적화하는 것을 의미한다. Graph Representation Learning의 ‘goal’이 ‘그래프의 구조적 정보를 잘 보존하는 것’이 되는 셈이다.         Node Embedding      위 그림의 그래프 $G = (V, E)$를 예로 들어 살펴보자.                     그래프 구조에 대하서 설명하자면, 5개의 node($v_1, … , v_5$)와 6개의 연결된 edge로 이루어진 그래프는 왼쪽 아래의 인접행렬 $A$로 표현되며 붉게 표시된 $v_4$와 연결된 3개의 node ${v_2, v_3, v_5}$의 연결은 성분 1로 표현된다. 인접행렬의 대각성분은 자기 자신과의 연결은 정의하지 않으므로 0이며, (i,j) 연결과 (j,i) 연결은 undirected graph에서는 구분되지 않으므로 대칭행렬이다.                        $|V| = 5$인 이 그래프에서, 각 노드 $v_i$를 차원 4의 저차원 벡터로 인코딩하는 mapping function $f : v_i \\rightarrow \\mathbb{R}^4$    을 학습하여, 그래프의 구조적 정보를 잘 보존하는 node embedding을 구하는 것이 바로 Graph Representation Learning인 것이다.                       그렇다면, learning의 goal인, 그래프의 구조적 정보를 잘 보존함을 판단하는 기준은 무엇인가?      구조적 정보는 유사도로 표현할 수 있다.   위 질문에 대한 답이, Graph Representation Learning의 핵심을 담는다. 직관적으로 생각할 때, ‘그래프의 구조적 정보’에 대한 gold label이 존재한다면 그 gold label에 가깝게 f를 learning하면 될 것이다. 하지만, 일반적으로 graph representaton learning 자체가 gold label이 없는 복잡한 자료구조에서 최대한 원래의 정보를 보존하는 representation을 구해내는 것에 의의를 두기에, 일반적으로 learning은 외부에서 주어지는 정답지가 없는, unsupervised learning의 형태로 이루어진다.   Learing의 핵심적인 아이디어는, 아래와 같다.      Embedding에서 node 간의 유사도(proximity) 가 원래 그래프와 비슷하게 보존된다면, 간접적으로 그래프의 중요한 구조 정보가 보존된다고 볼 수 있다.          유사도가 '거리'로 정의되는 상황   예를 들어보겠다. 지구에 살고 있는 영이($v_1$), 영희($v_2$), Mike($v_3$)를, 극좌표계의 단위벡터로 표현한다고 생각해보자. 훌륭한 representation이 되기 위해서는, 사람들 간의 유사도가, 극좌표계 상에서도 보존되어야 한다. 이 task를 단순명료화하기 위해서 아래와 같은 두 전제를 할 수 있다.      전제 1 - ‘사람들 사이의 유사도’는 사람들 간의 ‘거주하는 지역 간의 거리’가 가까울수록 크다.   전제 2 - 극좌표계에서 ‘벡터들 간의 유사도’는 벡터 사이의 각도가 작을수록 크다.   위 두 전제 하에 지구에서 영이와 영희가 옆집에 살고, Mike는 혼자 먼 섬나라에 산다는 것을 반영하면, 극좌표계 상에서 표현된 (영이, 영희) 사이의 각도가 좁고, (영희, Mike), (영이, Mike)의 각도가 큼은 합리적이다. 즉 지구에서 사람들 간의 ‘거주하는 지역 간의 거리’ 정보가, 극좌표계 상의 벡터 간 사잇각 정보로 reconstruct되었다고 볼 수 있다. 이는 ‘거리 정보’가 보존된다는 점에서 훌륭한 representation이다.   하지만, 위의 두 전제에 관해 드는 의문이 있다.     전제 1 - Node 간 유사도에 대한 의문 : 사람은 셀 수 없이 많은 각기 다른 특성들을 갖고 있다. 머리모양에서부터 목소리, 옷, 성별, 나이, 인종… 이 많은 특성들을 깡그리 무시하고 ‘거주하는 지역 간의 거리’가 ‘사람들 사이의 유사도’를 나타낸다고 가정하는 것이 합리적인가?   전제 2 - embedding space에서 보존되는 유사도 대한 의문 : 비슷한 맥락으로, 극좌표계의 두 임베딩 벡터 $z_i, z_j$의 유사도를 사잇각으로 정의하는 것이 합리적인가?         Learning Framework   결론부터 이야기하면, 위의 두 전제, 즉 Representation Learning에서 유사도의 정의는 연구자의 의도, 혹은 downstream task에 따라 달라진다. 따라서 representation 학습은 본질적으로, 어떤 그래프 상의 유사도를 어떤 방식으로 보존할 것인가에 대한 설계와 그 구현을 포함하는 과정이다.         Learning Scheme   위에서 다룬 그래프 $G = (V, E)$를 다시 갖고 와, 지금까지 설명한 내용을 수식으로 살펴보자.    1.유사도의 정의     연구자는 두 가지 유사도를 정의한다. 하나는 그래프 상의 유사도이며, 다른 하나는 embedding space 상의 유사도이다.            Pairwise Similarity Function(노랑)       그래프 $G = (V, E)$ 위에서 정의되는, 노드 간의 유사도를 나타내는 함수이며 아래와 같이 표현된다.   \\[s_G : V \\times V \\rightarrow \\mathbb{R}^+\\]      이 함수는 노드 $v_i$와 $v_j$ 사이의 그래프 기반 유사도를 정의한다.  예를 들어,              $s_G(v_i, v_j) = A_{ij}$일 경우, 유사도 자체는  노드 간 인접 여부 (adjacency matrix)를 의미한다.       Random walk 기반 representation learning에서 $s_G(v_i, v_j)$는 random walk에서의 공출현 확률로 정의될 수 있다.                Decoder Function(초록)       두 임베딩 벡터 $z_i, z_j$의 유사도를 계산하는 함수이며 아래와 같이 표현된다.   \\[\\text{DEC} : \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^+\\]      일반적으로 학습 parameter를 가지지 않으며 단순한  형태이다. 예를 들어,              $\\text{DEC}(z_i, z_j) = z_i^\\top z_j$일 경우, 두 벡터 간 inner product를 의미한다.       $\\text{DEC}(z_i, z_j) = -|z_i - z_j|^2$일 경우, 두 벡터 간 euclidean distance를 의미한다.            2.Learning의 과정      최적화되는 대상은 Encoder function이며, 학습의 목적은 Loss Function을 최소화하는 것이다.            Encoder Function       그래프 상의 노드 $v_i \\in V$를 $d$차원 벡터로 임베딩하는 함수이며 아래와 같이 표현된다.   \\[\\text{ENC} : V \\rightarrow \\mathbb{R}^d\\]      즉, 노드 $v_i$는 임베딩 벡터 $z_i = \\text{ENC}(v_i)$로 매핑된다. 이 함수는 학습 가능한 파라미터를 가지며, 학습 과정에서 최적화된다.            Loss Function       복원된 유사도 $\\text{DEC}(z_i, z_j)$와 원래 유사도 $s_G(v_i, v_j)$ 간의 차이를 최소화하는 손실 함수이다.       손실함수 \\(\\ell : \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}\\) 는 복원된 유사도와 실제 유사도 간의 차이를 측정하며, 전체 학습 손실은 다음과 같이 정의된다.   \\[\\mathcal{L} = \\sum_{(v_i, v_j) \\in \\mathcal{D}} \\ell\\left(\\text{DEC}(z_i, z_j), s_G(v_i, v_j)\\right)\\]      여기서 $\\mathcal{D}$는 학습에 사용되는 노드 쌍들의 집합이다.               정리하면   위 프레임워크를 공통적인 틀로 가지는 Graph Representation Learning은 결국 아래의 두 요소가 본질이다.            유사도 설정 : $s_G$(그래프 상의 유사도)와 Decoder(Decoding한 유사도)를 어떻게 정의하는가?     Learning의 과정 : 어떤 구조의 encoder를 사용하는가?, loss 함수는 어떻게 정의하는가?       위 요소의 차이에 따라 Graph Representation Learning의 Method의 세부 분류가 나누어진다.   사실, 스탠포드 CS224w의 강의자이시자 위 프레임워크를 제안하신 Jure Leskovec 교수님께서 지적하셨듯, GRL 방법들은 다양한 벤치마크와 모델 개념들이 산발적으로 존재하여 이론적 통일성이 결여되어 있다. 각기 다른 하위 분야에서 독립적으로 연구되어 온 GRL의 task 중심 평가에서 벗어나, 본질적으로 우리가 어떤 graph 구조를 representation에 담고자 하는지, 어떻게 그것을 encoding해야 하는지, latent space에 어떤 제약을 둘 것인지에 대한 체계화된 개념론적 논의가 더 활발히 이루어지길 소망한다.     참고문헌     Khoshraftar, Shima, and Aijun An. “A survey on graph representation learning methods.” ACM Transactions on Intelligent Systems and Technology 15.1 (2024): 1-55.   Ju, Wei, et al. “A comprehensive survey on deep graph representation learning.” Neural Networks 173 (2024): 106207.   Hamilton, William L., Rex Ying, and Jure Leskovec. “Representation learning on graphs: Methods and applications.” arXiv preprint arXiv:1709.05584 (2017).  ","categories": ["Graphs"],
        "tags": ["Graphs","Representation learning"],
        "url": "http://localhost:4000/graphs/Representation-Learning-on-Graphs/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "매수 매도 규칙",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   성장하는 자본주의 시스템에서 통화량의 상승과 통화가치의 하락은 자연스러운 수순이다. 따라서 현금을 갖고 있기보다 현물 자산으로 바꾸어 갖고 있는 것이 현명한데, 기업의 가치를 반영하는 주식에 투자하는 방식은 그 중 하나이다.   그러나 파는 사람과 사는 사람이 항상 같이 존재하는 주식시장에서, 누군가가 수익을 얻는다면 동시에 누군가는 손실을 입기 마련이고, 기업 가치에 대한 정보의 열위에 있는 일반투자자는 후자에 속할 확률이 높다. 특히 이익을 확정짓고 싶어하고 손실을 기피하는 사람의 심리는 예측 가능하기에 감정적인 일반투자자는 더욱 세력의 이익실현에 휘둘리기 쉽다. 매수와 매도의 기준이 명확해야 하는 이유이다.   조금 더 개인적인 이야기를 해보자면, 나에게는 지금 순간의 나의 행복이 꽤 중요한 가치이다. 차트의 단기적 오르내림에 나의 행복도가 흔들리지 않기 위해서는 ‘내가 맞게 투자하고 있다.’는 믿음이 있어야 하더라. 믿음의 영역에 정답은 없고 죽기 전까지 주식이 떨어지기만 하더라도 언젠간 오른다는 믿음이 있다면 행복하기야 하겠지만… 그래도 이왕이면 믿는대로 이루어지면 더 좋으니 시장의 원리를 바탕으로 나의 매수, 매도 규칙을 정하고자 한다.       1. 매수      대상            내가 관심이 있고, 장기적인 상승의 근거가 명확한 분야의 회사       단기적인 이벤트로 주가 상승이 예측되는 회사 (다만 높은 비중으로 투자하지는 말자)                    임상 승인           관세 / 정책의 변화           단기적이고 비합리적인 하락 (다시 오를 것이 확실할 때)                                방식       상승 추세의 눌림목일 때 구매             상승 추세는 binary wave 7개 지표로 기계적으로 판단       최근 급격한 상승이 이미 이루어졌다면, 관심을 갖되 당장 매입하지는 말자              2.매도      대상            상승추세가 깨진 경우       급격한 상승으로 조정이 예상될 때 혹은 너무 매력적인 다른 종목이 생겼을 때           방식            7개 지표 중 2개 이상 깨졌을 경우 : 전량 분할 매도       7개 지표 중 1개가 깨졌을 경우 : 절반 분할 매도              3.분배      현금은 10% 보유   안정적인 기업 / 도전적인 기업에 분산투자하고자 한다. 투자에 조금 더 경험과 확신이 생기면, 가치가 확실한 기업에 집중투자하는 방식으로 바꾸어나가야겠다.  ","categories": ["Finance"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/finance/%EB%A7%A4%EC%88%98%EB%A7%A4%EB%8F%84%EA%B7%9C%EC%B9%99/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "투자의 목적",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   자산을 갖고 있는 한, 투자는 필연적이다. 아무 것도 하지 않고 예금에 나의 돈을 넣어둔다면, 이 또한 한국의 원화라는 가치에 돈을 투자하고 있는 셈이다. 투자에 대한 고민의 과정을 기록한다.       1. 투자의 목적   무엇이든, 잘하기 위해서는 노력과 시간이 필요하다.   투자 역시 마찬가지이다.   단기투자를 잘하기 위해서는 미시적인 사람의 심리와 단기적인 이벤트, 차트의 흐름을 공부하고 살펴야 한다.   장기투자를 잘하기 위해서는 거시적인 세상의 흐름을 예민하게 살펴야 한다.   인생은 짧고 젊음은 더 짧기에, 지금 시기의 나의 노력과 시간을 어디에 얼마나 배분해야 할지 고민할 필요가 있었고   고민의 결론은, 20대 초반에서 중반을 향하고 있는 지금 시점에는, 세상의 가치를 살피기보다 나의 가치를 키우는 데에 집중하고 싶다는 것이었다.   그래서 지금 나의 투자의 목적은, 최소한의 노력을 들여 적어도 나의 가치를 키우는 것에 방해되지 않는 선에서 자산을 운용하는 데에 있다.   조금 더 구체적으로 말하면, 공부하는 와중 먹고 싶은 것을 먹고, 가끔 놀러가고 싶을 때 갈 수 있고, 소중한 주변 사람들을 챙길 수 있을 정도로 현명하게 자산을 관리하는 것이 목표이다.   타인이 실현할 가치를 예측하고 투자하여 10억을 만드는 것보다,   나의 잠재력을 키워, 내가 실현하는 가치에 투자자가 10억을 아깝지 않게 지불하게 만드는 것이   적어도 내 인생에 있어서는 내가 나아가고 싶은 방향이다.         2. 어떻게 투자할 것인가   그래서 가만히 있겠다는 것이냐, 그럴 수는 없다.   권력자가 돈을 찍어낼 수 있는 사회구조 상 현금은 가치 하락이 예견된 자산이다. 투자를 달리기에 비유한다면, 속도에서의 완급조절은 자유이지만 올바른 방향 설정은 필요하다고 생각한다.   아래는 내가 고민한 어떻게 투자할 것인가에 대한 나의 방향성이다.      자산의 가치는 사람들의 기대를 반영한다. ‘사람들이 곧 기대할, 그러나 지금은 저평가된’ 가치 있는 기업을 찾는다.   그러나 기업을 찾는 데에 있어서 거인의 어깨에 올라서는 것을 주저하지 않는다.   1.에서 찾은 후보에 대해서 기술적 분석으로 매수/매도 시점을 정한다. 그러나 정확한 타이밍을 예측하려 하기보다, 추세를 확인하고 분할매매한다. 구체적으로, 상승추세의 눌림목에서 매수하고, 하락추세에 진입했을 때 매도한다.   포트폴리오를 관리하는 데에 시간적, 감정적 소모를 최소화한다.            장이 열리는 시각에 오르고 내리는 차트를 들여다보지 않는다. 후보 기업과 주가의 추세를 가시화하여, 정한 시점에 확인한다.       차트의 오르내림에 감정을 소모하지 않고 정한 규칙에 따라 매수, 매도한다.          ","categories": ["Finance"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/finance/%ED%88%AC%EC%9E%90%EC%9D%98-%EB%AA%A9%EC%A0%81/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "믿음의 영역",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.        지식이 믿음에 대해서 우위를 보이는 요소인 객관적 사실성과 보편성은 판단과 결정을 통한 행위의 영향력과 관련해서 의외로 짧은 유효거리를 가질 뿐이다. 다소간의 확실한 요소들이 유입될 뿐인 미래의 전개는 언제나 우연성과 불확실성을 포함한다. 반면에 믿음은 다양한 삶의 연관 안에서 형성된 개인적 출발점의 확신을 통해 지식이 달성할 수 없는 총체적인 실재 전체와의 연관을 실현해 내고 인간을 고무하고 희망을 주는 힘이다.     양대종. “지식과 믿음의 연관에 대한 고찰.” 철학연구 (2023): 119-145. 중.      삶이 거시적으로 좋은 방향으로 흘러가고 있다는 믿음은, 예측하지 못한 변수를 회피하지 않고 마주할 수 있게 만들며   노력이 어떤 순간에 내게 돌아올 것이라는 믿음은, 결과의 불확실함에 흔들리지 않고 최선을 다할 수 있게 만든다.   하고 있는 일이 옳고 선하다는 믿음은, 누군가의 비난에 상처받지 않고 당당할 수 있는 힘을 주며   타인의 선의와 사랑에 대한 믿음은, 함께 사는 세상에서 행복의 근원에 가깝다고 생각한다.       ‘그냥’이라는 이유는 비논리적이지만 역설적으로 그러해서 반박불가하며 강력하다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/07.-%EB%AF%BF%EC%9D%8C%EC%9D%98-%EC%98%81%EC%97%AD/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "우연 속의 필연",
        "excerpt":"  원칙은 바뀔 수 있음이 원칙이다.   Introduction   세상은 대체로 모호하다.           정책은 정의로움과 부당함의 속성을 동시에 가진다.            객체는 상태의 중첩에 놓여 있으며 입자성과 파동성을 동시에 가진다.            가설 검증에서 채택되는 가설은 언제나 오류를 범할 확률을 동반한다.       세상은 흑과 백이 아닌 ‘덜 회색’과 ‘더 회색’에 가깝고, 벌어지는 일들은 예외 없는 인과가 아닌 상관성과 확률에 기반한다.   완전무결함은 사람이 쌓아 올린 믿음이 영역이 아닌 실재의 영역에서는 찾아보기 어렵다.       불확실함을 인정하며 불안하지 않을 수 있는가   하지만 우리는 대체로 완전무결함을 추구한다. 0과 1의 명확한 구분에서 안정감을 느끼며 확실한 미래를 예측하고 싶어한다. 끊임없이 선택해야 하는 인간의 조건을 고려할 때 이는 자연스러운 속성이다.   그러나 불확실함을 부정하며 완전무결함을 추구하는 것은 종종 부작용을 낳는다. 부정하는 과정에서 무시한 - 주로 크기가 작은 - 속성들이 사실은 중요했거나, 시간이 지남에 따라 중요해지기도 하기 때문이다.   불확실함을 부정하는 믿음은 독단으로 변모하기 쉽다.       행복한 시지푸스   철학자 알베르 카뮈는 돌이 굴러 떨어질 것을 앎에도 끊임없이 돌을 밀어올리며 삶의 의미를 찾는 ‘행복한 시지푸스’를 이야기한다.   카뮈가 제시하는 삶의 태도는 다음 두 가지와는 구별된다:      부정적 낙관주의: 돌이 떨어지는 것을 부정하며 행복하게 돌을 밀어올리는 삶   허무주의: 돌이 떨어지는 것을 알고 아예 돌을 밀어올리지 않는 삶   카뮈는 이 둘이 아닌, 돌이 떨어지는 것을 앎에도 행복하게 돌을 밀어올리는 삶을 살아야 한다고 말한다.   삶의 이유를 한참 고민하던 예과 2학년 시기, 나는 시지푸스의 행복이 이해가 되지 않았었다. 어떻게 행복할 수 있지 싶었다.       우연 속의 필연   그러나 생각해보면, 사실 우리는 누구나 시지푸스와 같은 처지이다.   우리는 스스로가 어떤 우연으로 지금 당장, 혹은 언제든 삶을 마감할 수 있음을 안다. 관계가 영원하지 않을 수 있음도 안다. 하지만 몇 시간 뒤에, 혹은 내일 당장 기막힌 확률로 세상을 떠날 것을 걱정하며 사는 사람은 거의 없다. 우리는 불안함 없이 죽음의 위험을 유보하며 삶에 충실하게 살아간다. 영원을 약속하고 사랑하며 살아간다.     이러한 태도는 다른 차원의 불확실함에도 적용될 수 있다고 생각한다.   내가 꿈꾸는 미래가 불친절한 변수로 인해 이루어지지 않을 수 있음을 알면서도, 내가 틀릴 수 있는 가능성을 인정하면서도,   불확실함을 부정하지도, 그것에 압도되지도 않으면서, 현재의 선택과 행동에 온전히 투신하는 삶을 살고 싶다.   우연을 앎에도 필연을 믿는 삶을 살고 싶다.  ","categories": ["Thoughts"],
        "tags": ["에세이"],
        "url": "http://localhost:4000/thoughts/08.-%EC%9A%B0%EC%97%B0-%EC%86%8D%EC%9D%98-%ED%95%84%EC%97%B0/",
        "teaser": "http://localhost:4000/"
      },{
        "title": "Two approaches of embedding nodes - Shallow / NN-based",
        "excerpt":"  Introduction   앞선 글에서 설명했듯, Graph Representation Learning은 그래프의 요소(\bnode, \bedge, \b\bsubgraph)를 저차원 벡터로 매핑하는 일이다. 많은 survey가 여러 요소 중에서도 node embedding에 초점을 맞추는 이유는 다음과 같다.           Edge/subgraph \bembedding이 node \bembedding의 후처리로 귀결되는 경우가 많다.       예를 들어             \bedge $(v_i,v_j)\\in E$에 대해 \bnode \bembedding $\\mathbf{z}_i,\\mathbf{z}_j$가 주어지면 hadamard/mean/weighted-L1·L2 같은 이항 연산으로       \\[\\mathbf{z}_{(i,j)}=\\mathbf{z}_i\\odot\\mathbf{z}_j\\]      처럼 edge \bembedding을 만든다.              \b\bsubgraph $\\mathcal{G}[\\mathcal{S}]$의 \bembedding도 보통 $\\mathcal{S}\\subset\\mathcal{V}$에 포함된 \bnode \bembedding을 집계(average/attention 등)해서 얻는다.       \\[\\mathbf{z}_{\\text{subgraph}}=\\frac{1}{|\\mathcal{S}|}\\sum_{v_i\\in\\mathcal{S}}\\mathbf{z}_i\\]           Edge/Subgraph \bembedding을 직접 학습하더라도 절차는 node \bembedding 학습과 본질적으로 유사하다.             예를 들어 타깃 \b\bsubgraph의 모든 node에 연결된 dummy \bnode를 추가하고 그 \bnode의 \bembedding을 학습 대상으로 두면 사실상 node embedding 학습과 거의 동일한 형태가 된다.           따라서 node embedding을 이해하면 edge/subgraph embedding도 같은 틀 안에서 상당 부분 설명된다. 이하에서는 \b\b\bencoder를 어떻게 정의하는지에 따라 node embedding을 shallow embedding과 NN-based embedding으로 비교해 정리한다. 본 글은 “Representation Learning on Graphs” (2017) 에서 제시한 개념을 바탕으로 필자의 해석을 더해 작성하였다.     Review : Learning framework   ‘그래프에서 가까운 \bnode는 \bembedding 공간에서도 가깝다’는 structural assumption(구조 가정) 아래, 다음 세 가지를 설계한다.           그래프 상의 유사도 $s_G$   \\[s_G:\\mathcal{V}\\times\\mathcal{V}\\to\\mathbb{R}^+,\\qquad s_G(i,j)\\in\\{A_{ij},\\ k\\text{-hop},\\ \\text{랜덤워크 공출현확률}\\]           \bEmbedding 공간의 \bdecoder $\\mathrm{DEC}$   \\[\\mathrm{DEC}:\\mathbb{R}^d\\times\\mathbb{R}^d\\to\\mathbb{R},\\quad \\text{예: }\\ \\mathbf{z}_i^\\top\\mathbf{z}_j,\\ -\\lVert\\mathbf{z}_i-\\mathbf{z}_j\\rVert_2^2,\\ \\sigma(\\mathbf{z}_i^\\top\\mathbf{z}_j)\\]           \b\b\bEncoder $\\mathrm{ENC}$와 loss function(learning objective)       \\[\\mathbf{z}_i=\\mathrm{ENC}(v_i),\\qquad   \\mathcal{L}=\\sum_{(i,j)\\in\\mathcal{D}}\\ell\\!\\Big(\\mathrm{DEC}(\\mathbf{z}_i,\\mathbf{z}_j),\\ s_G(i,j)\\Big)\\]  여기서 구조 가정은 ‘무엇을 가깝게 유지할지’에 대한 inductive bias, 즉 네트워크 구조에 관한 모델의 prior다.   $s_G$를 어떻게 정의할지(무엇을 ‘유사’로 볼지), $\\mathrm{DEC}$를 어떻게 둘지(\bembedding에서 유사를 어떻게 수치화할지), $\\mathrm{ENC}$를 어떻게 설계할지(그 유사를 재현하도록 표현을 만들지)에 이 가정이 명시적·암묵적으로 들어 있다.          Two approaches of embedding nodes   위와 같은 $\\mathrm{ENC}$–$s_G$–$\\mathrm{DEC}$ 프레임워크 안에서, 학습이 일어나는 층위에 따라 접근을 두 가지로 나눌 수 있다.      Shallow : 미리 정한 $s_G$에 맞춰 \bnode별 \bembedding 자체를 직접 최적화한다.   NN(GNN)-based: 유사도를 만들어내는 연산 자체(메시지 패싱/오토\b\b\bencoder 등)를 학습해 \bembedding을 간접적으로 만든다. 파라미터는 \bnode 간 공유된다.     1. Shallow embedding   Shallow embedding은 node ID를 임베딩 행렬의 열(column)에 직접 매핑하는 방식이다.   \\[Z \\in \\mathbb{R}^{d \\times |\\mathcal{V}|}, \\qquad \\mathrm{ENC}(v_i) = Z_{\\cdot i} = \\mathbf{z}_i\\]  Encoder 구조가 단순하기 때문에, “무엇을 보존할지”에 대한 선택이 거의 전적으로 그래프 상의 유사도 정의 $s_G$와 Decoder 설계에 담긴다.     1-1. Factorization-based approaches           무엇을 보존할지 ($s_G$ 설정)  유사도 행렬 $S$는 인접성, 근접성, 전파 도달성 등 다양한 그래프 속성을 반영하도록 정의할 수 있다. 예시는 다음과 같다.   \\[S = A \\quad \\text{(adjacency)}, \\qquad S = A^k \\quad (k\\text{-hop}), \\qquad S = \\text{Katz}, \\ \\text{RPR}, \\ \\text{Jaccard} \\ \\text{등}\\]      $S$의 정의에 따라, embedding이 강조하는 구조적 특성(커뮤니티, 국소 근접성, 전파 가능성 등)이 달라진다.            Embedding에서 어떻게 읽을지 (Decoder)       대표적인 Decoder 정의는 다음과 같다.   \\[\\mathrm{DEC}(\\mathbf{z}_i, \\mathbf{z}_j) = \\mathbf{z}_i^\\top \\mathbf{z}_j \\quad \\text{또는} \\quad- \\lVert \\mathbf{z}_i - \\mathbf{z}_j \\rVert_2^2\\]           Loss function (learning objective)                       Decoder가 내적(inner product)일 경우 \\(\\min_{Z} \\ \\lVert S - Z^\\top Z \\rVert_F^2\\)                        Decoder가 음의 유클리드 거리일 경우 \\(\\sum_{i,j} W_{ij} \\lVert \\mathbf{z}_i - \\mathbf{z}_j \\rVert_2^2  \\ = \\ 2\\, \\mathrm{tr}(Z L Z^\\top)\\)                     1-2. Random walk-based approaches           무엇을 보존할지 ($s_G$ 설정)       랜덤 워크를 통해 생성한 노드 시퀀스에서 공출현(co-occurrence) 강도 또는 확률을 노드 간 유사도로 정의한다.             DeepWalk: 편향 없는(unbiased) 랜덤 워크를 사용.       node2vec: 하이퍼파라미터 $p, q$로 BFS/DFS 성향을 조절하여, 국소적 조밀성(locality)과 구조적 역할(role) 간의 균형을 조정.                Embedding에서 어떻게 읽을지 (Decoder)       Factorization 기반 방법과 유사하게, 내적 기반 Decoder를 사용하되, 유사도 행렬 S가 아니라 랜덤 워크 확률을 근사하도록 학습한다.  즉, 다음 조건이 만족되도록 embedding을 학습한다.   \\[\\mathrm{DEC}(\\mathbf{z}_i, \\mathbf{z}_j)= \\frac{\\exp(\\mathbf{z}_i^\\top \\mathbf{z}_j)}      {\\sum_{v_k \\in \\mathcal{V}} \\exp(\\mathbf{z}_i^\\top \\mathbf{z}_k)} \\ \\approx \\ p_{G,T}(v_j \\mid v_i)\\]      여기서 $p_{G,T}(v_j \\mid v_i)$는 그래프 $G$에서 노드 $v_i$에서 시작해 길이 $T$의 랜덤 워크를 수행할 때 노드 $v_j$를 방문할 확률이며, 일반적으로 $T \\in { 2, \\dots, 10 }$ 범위로 설정된다.            Loss function (learning objective)       위 확률 근사를 위해 다음 교차 엔트로피 손실을 최소화한다.   \\[\\mathcal{L} = \\sum_{(v_i, v_j) \\in D} - \\log \\left( \\mathrm{DEC}(\\mathbf{z}_i, \\mathbf{z}_j) \\right)\\]      여기서 $D$는 각 노드 $v_i$에서 시작하는 랜덤 워크로부터 샘플링된 $(v_i, v_j)$ 쌍들의 집합이다. 그러나 위 식의 분모 계산은 $O(|\\mathcal{V}|)$ 시간이 소요되므로, 전체 학습 비용은 $O(|D||\\mathcal{V}|)$로 매우 크다. 이를 해결하기 위해:              DeepWalk의 경우 : Hierarchical softmax를 사용하여, 이진 트리 구조로 정규화 항 계산을 가속한다.       Node2vec의 경우: Negative sampling 사용하여, 전체 노드 집합 대신 무작위로 선택한 소수의 negative sample로 분모를 근사한다.                가장 대표적인 random walk basedembedding method를 다루었는데, Deepwalk와 node2vec 외에도 LINE, HARP 등의 다양한 방법이 있다. 이 역시 다음에 자세히 다루겠다.            2. NN-based embedding   Shallow가 node-id만 입력으로 받아 바로 embedding을 lookup하는 것과 달리, NN-based는 \bnode 특징 $\\mathbf{x}_i$와 이웃 $\\mathcal{N}(i)$를 입력으로 받아 처리하는 네트워크를 학습한다.   2-1. Neighborhood Autoencoder (DNGR, SDNE)   각 노드 $i$마다, 이웃 분포(neighborhood distribution) 를 나타내는 벡터  $\\mathbf{s}_i \\in \\mathbb{R}^{|\\mathcal{V}|}$를 생성한 뒤, 이를 Autoencoder로 복원한다. 여기서 $\\mathbf{s}_i$는 행렬 $S$의 $i$번째 행에 해당하며, $S$는 노드 간 유사도 $s_G(v_i, v_j)$를 담고 있다.  즉, $\\mathbf{s}_i$는 노드 $v_i$가 그래프 내 모든 다른 노드와 가지는 유사도를 포함하는 고차원의 벡터다.   Learning objective는 다음과 같다.   \\[\\min_{\\theta,\\phi} \\ \\sum_i  \\left\\lVert \\mathbf{s}_i - \\mathrm{DEC}_\\phi\\big(\\mathrm{ENC}_\\theta(\\mathbf{s}_i)\\big) \\right\\rVert_2^2\\]    Shallow embedding과 달리 파라미터가 노드 간 공유되어 연산이 효율적이다. 하지만 입력 차원이 $|\\mathcal{V}|$에 고정되어 있어 대규모 그래프 적용이 어려우며, 새로운 그래프에 대한 적용도 제한적이다. ( transductive )   2-2. Neighborhood Aggregation (GCN/GraphSAGE/GAT)   Message Passing으로 이웃 표현을 aggregate해 자신의 표현과 결합하는 연산을 여러 층 반복한다.       참고로, Message Passing은 “Neural message passing for quantum chemistry.”논문에서 처음 GNN의 framework를 통합적으로 설명하기 위해 제시한 개념으로, 이후 많은 논문들에서 해당 개념을 차용하여 GNN을 설명하기 시작하였다.  딥마인드의 Petar Veličković는 최신의 ‘beyond message passing’으로 여겨졌던 GNN 기법도 본질적으로 message passing framework으로 모두 설명할 수 있다고  “Message passing all the way up”  에서 주장하였다.     연산 과정은 다음과 같다.   \\[\\mathbf{h}_i^{(0)} = \\mathbf{x}_i, \\quad \\mathbf{h}_i^{(k)} = \\mathrm{COMBINE}^{(k)}\\Big(   \\mathbf{h}_i^{(k-1)}, \\   \\mathrm{AGG}^{(k)}\\{\\mathbf{h}_j^{(k-1)} : j \\in \\mathcal{N}(i)\\} \\Big), \\quad \\mathbf{z}_i = \\mathbf{h}_i^{(K)}\\]     초기 임베딩은 입력 특징 $\\mathbf{x}_i$로 설정한다.   각 단계에서 이웃 노드 임베딩을 집계($\\mathrm{AGG}$)한다.   집계된 이웃 표현과 자신의 이전 단계 표현을 결합($\\mathrm{COMBINE}$)한다.   이 과정을 $K$번 반복하여 최종 임베딩 $\\mathbf{z}_i$를 얻는다.   위 encoder 연산에서 COMBINE function과 AGG function을 어떻게 정의하는지에 따라 GNN의 세부 방법론(GCN, GraphSAGE, GAT 등) 들이 나누어진다.       GNN은 구조 가정을 $\\mathrm{ENC}$가 담당하므로, $\\mathrm{DEC}$와  Loss function(learning objective) 은 태스크에 맞춰 유연하게 고른다.     링크 예측 : $\\mathrm{DEC}(\\mathbf{z}_i,\\mathbf{z}_j)=\\sigma(\\mathbf{z}_i^\\top\\mathbf{z}_j)$, negative sampling   \bnode 분류 : $\\hat{\\mathbf{y}}_i=\\mathrm{softmax}(W\\mathbf{z}_i)$, cross-enthropy   그래프/\b\bsubgraph 관련 supervised task: $\\mathrm{READOUT}$으로 node embedding을 post processing한 후 태스크별 $\\mathrm{DEC}$ 사용       이 방법은 가장 진보한, 현재에도 활발히 연구되고 있는 graph representation 방식으로, 흔히 GNN을 이야기하면 보통 neighborhood aggregation을 바탕으로 한 이 message passing을 상정한다.   파라미터 공유로 모델 크기가 그래프 크기와 관계 없이 효율적으로 작게 유지된다. \b\bNode feature/edge weight 등 그래프 관련 meta information을 자연스럽게 활용할 수 있으며, 처음 보는 그래프에도 적용이 수월하다. ( inductive )     마무리하며           Graph representation learning의 framework는, (1) 무엇을 유사로 볼지($s_G$), (2) \bembedding에서 그 유사를 어떻게 읽어낼지($\\mathrm{DEC}$), (3) 그 유사를 재현하도록 어떤 연산을 학습할지($\\mathrm{ENC}$) 를 설정(구조 가정) 하는 것이 핵심이다.              Shallow는 (1) 유사도를 정의하고(S 혹은 공출현확률) 그에 맞춰 (2) embedding 열의 내적이 비슷하도록 (3) embedding 열을 각 node id마다 따로따로 lookup한다.       Neighborhood Autoencoder는 (1) node마다 이웃 node의 정보를 나타내는 $\\mathbf{s}_i$를 정의해, (3) 오토인코더를 이용해 (2) $\\mathbf{s}_i$를 복원하도록 효율적으로 학습한다.       Neighborhood Aggregation(GNN) 은 (1)(3) “무엇을 보존할지”가 message passing network에 녹아 있으며, 태스크에 따라 (2) $\\mathrm{DEC}$/loss를 유연하게 붙일 수 있다.           구체적 방법론과 수식 유도는 후속 글에서 더 다룰 예정이다.  ","categories": ["Graphs"],
        "tags": ["Graphs","Representation learning"],
        "url": "http://localhost:4000/graphs/Two-approaches-of-embedding-nodes-Shallow-embedding-and-NN-based-embedding/",
        "teaser": "http://localhost:4000/"
      }]
