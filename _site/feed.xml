<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-21T22:31:13+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">DaRe_jin’s Blog</title><subtitle>내실 있는 낙관</subtitle><author><name>DaRe_jin</name></author><entry><title type="html">섬세한 솔직함</title><link href="http://localhost:4000/thoughts/09.-%EC%84%AC%EC%84%B8%ED%95%9C-%EC%86%94%EC%A7%81%ED%95%A8/" rel="alternate" type="text/html" title="섬세한 솔직함" /><published>2025-09-21T00:00:00+09:00</published><updated>2025-09-21T00:00:00+09:00</updated><id>http://localhost:4000/thoughts/09.%20%EC%84%AC%EC%84%B8%ED%95%9C%20%EC%86%94%EC%A7%81%ED%95%A8</id><content type="html" xml:base="http://localhost:4000/thoughts/09.-%EC%84%AC%EC%84%B8%ED%95%9C-%EC%86%94%EC%A7%81%ED%95%A8/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>

<h1 id="introduction">Introduction</h1>
<hr />

<blockquote>
  <p>“시험 잘 봤어?”라는 질문에,</p>
  <ul>
    <li>“응, 꽤 잘 봤어.”</li>
    <li>“아니, 못 봤어.”</li>
  </ul>

  <p>를 같은 무드로 망설임 없이 이야기할 수 있는 사람이 되고 싶다고 생각했다.</p>
</blockquote>

<p><strong>겸손함</strong>과 <strong>자기비하</strong>, <strong>자신감</strong>과 <strong>자랑</strong>, <strong>솔직함</strong>과 <strong>무례함</strong>의 경계는 모호하다.</p>

<p>더욱이 그 경계는 행하는 내가 아닌, 받아들이는 사람의 상황과 맥락에 의해 정의되기도 한다.</p>

<hr />
<p><br /></p>

<h1 id="인정-욕구">인정 욕구</h1>
<hr />
<p>인간은 본능적으로 타인에게 인정받을 때 행복감을 느낀다. 자랑과 뽐냄 또한 이러한 인정 욕구의 한 형태일 것이다.</p>

<p>타인에게 결핍되지 않은 것, 혹은 타인이 그리 욕망하지 않는 것을 자랑하는 사람은 인정받기 훨씬 수월하다.</p>

<blockquote>

  <p>가령 이런 것이다.</p>

  <p>종이접기에 푹 빠져있는 사람이, 몇 날 며칠 밤을 새워 아주 멋진 작품을 완성했음을 자랑할 때, 우리는 어떤 상처도 받지 않고 그의 노력과 열정, 작품의 멋있음을 인정할 수 있다.</p>

</blockquote>

<p>‘나도 가지고 싶은 것’을 자랑하는 다른 이를 바라볼 때, 사람은 본인의 결핍과 마주한다. 인정하기보다는 상처를 받는다.</p>

<hr />
<p><br /></p>

<h1 id="겸손함이-미덕이-되는-사회">겸손함이 미덕이 되는 사회</h1>
<hr />
<p>겸손함이 미덕이 되는 사회에서, 인정 욕구는 두 가지 방향으로 고도화되었다.</p>

<blockquote>
  <ol>
    <li>내가 애써 포장하지 않아도 드러나는 정량적인 지표를 갖추는 일: 학벌, 성적, 실적, 자산</li>
    <li>타인의 결핍을 헤집지 않는, ‘선으로 통용되는 성향’을 드러내는 것: 이타심, 책임감, 친절함</li>
  </ol>
</blockquote>

<p>정교하게 정제된 인정 욕구는 행위자의 자랑을 표면적으로 드러내지 않기에, 사람들에게 훨씬 더 담백하게 받아들여진다.</p>

<p>위 두 가지를 서툴게 수행하여 1에서 포장이 더해지거나, 2에서 진정성이 결여된 모습이 보일 때 사람들은 종종 분노한다. 은근히 상대를 속이거나 얕보는 것을 의미하는 ‘기만’이라는 용어는 이런 분노를 칭하는 것이리라.</p>

<hr />

<p>공동체에서 인정받고, 타인에게 존중받고 싶은 마음은 문화와 역사의 원동력이라고 해도 과언이 아니다. 이타심과 책임감, 친절함을 드러내는 것 역시 그 동기가 인정 욕구이든, 선한 마음이든 무엇이 중요할까 싶기도 하다.</p>

<p>하지만 (1) 인정받기 위한 노력과 (2) 나를 가꾸는 일을 동일시하기에는 애매한 지점들이 있다.</p>

<p>아무도 보지 않고 있을 때의 나의 모습은 후자를 통해서만 발전한다.</p>

<p>‘타인으로부터의 인정’ 이전에 ‘스스로부터의 인정’은 나의 모든 모습을 판대에 올려놓고 이루어진다. 보이는 모습과 ‘나’의 괴리는 나의 결핍과 끊임없이 마주하게 만들며 스스로를 상처 입힌다.</p>

<hr />
<p><br /></p>

<h1 id="unconditional-positive-regard">Unconditional positive regard</h1>
<hr />
<p><strong>존재론적인 인정</strong>이 주는 힘은 강력하다. 서투름과 모자람도 존재의 일부로서 받아들여지기 때문이다. 타인이 나를 존재론적으로 인정해줄 것이라는 믿음은, 나를 있는 그대로 솔직하게 내보일 수 있게 한다.</p>

<p>다만 나를 내보이는 과정에서, 혹 타인의 결핍에 손을 대어 그를 상처 입히지는 않을지, 완급 조절에 섬세할 필요가 있다. 배려의 영역이다.</p>

<p><a href="https://dictionary.apa.org/unconditional-positive-regard">“무조건적 긍정적 존중(unconditional positive regard)”</a>은 사랑과 연민의 다른 이름이라고 생각한다.</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Thoughts&quot;]" /><category term="에세이" /></entry><entry><title type="html">격을 갖춤</title><link href="http://localhost:4000/bridges/%EA%B2%A9%EC%9D%84-%EA%B0%96%EC%B6%A4/" rel="alternate" type="text/html" title="격을 갖춤" /><published>2025-08-21T00:00:00+09:00</published><updated>2025-08-21T00:00:00+09:00</updated><id>http://localhost:4000/bridges/%EA%B2%A9%EC%9D%84%20%EA%B0%96%EC%B6%A4</id><content type="html" xml:base="http://localhost:4000/bridges/%EA%B2%A9%EC%9D%84-%EA%B0%96%EC%B6%A4/"><![CDATA[<hr />

<p class="notice--info">S와 이야기를 하던 중 공유 받은 S의 생각입니다.</p>

<h3 id="1">1.</h3>
<blockquote>
  <ul>
    <li>번아웃 증후군: 장기 피로로 인한 열정 상실</li>
    <li>권태기: 어떤 일이나 상태에 시들해져서 생기는 게으름이나 싫증</li>
  </ul>
</blockquote>

<p>위 같은 증후군은 단순히 많은 업무에서 오는게 아니다. 누군가는 한가지의 일을 쉽게 포기하고 자신이 처한 환경의 문제를 외치지만 다른 누군가는 만가지의 일을 버젓이 해내고 힘든 내색 하나 없다. 어느샌가 번아웃 증후군은 포기를 변호하기 시작했고 권태기는 식어버린 열정, 식어버린 사랑을 정당화하는 수단이 되었다. 내가 처한 환경, 시기가 내가 지금까지 받던 부담이라는 짐을 대신 받아주고 나는 홀가분하게 그 사명감에서 벗어난다. 이러한 용어들에 대한 사람들의 사회적 옹호는 환경과 시기의 의인화를 만들었고 편하게 환경과 시기에게 책임을 넘긴다. 의인화는 문학적 허용이기에 실제로는 아무도 책임을 대신 받아주지 못한다.</p>

<h3 id="2">2.</h3>

<p>‘격’은 다름말로 품위다. 인간이 인간다운 품위를 유지한다는 건 원초적인 생존 이상의 사치다. 사치를 부리지 않는 자를 비판할 권리는 없다. 남에게 사치를 부리라고 권고할 이유는 더더욱 없다. 그렇지만 현대 사회에서 인간다운 생존은, 원초적인 생존을 의미하지 않게 된지 오래이다. 사치가 아닌 생존의 영역에 들어간 ‘격’은 그렇지 못한 자들을 자연선택시키는 데에 일조한다고 보인다.</p>

<h3 id="3">3.</h3>

<p>인격을 갖춘 자는 부담을 받아들이는 능력이 있다. 이때 이런 부담들은 때로는 서로 상충한다. 하지만 그럼에도 그는 상충하는 부담들 마저 받아들일 수 있다. 그는 여유로워 보이고 걱정이 없어 보인다. 그와 이야기하는 사람들은 그가 수많은 고민이 있는걸 알아채기는 커녕 본인의 고민마저 잊게 된다. 그의 여유로움은 남들의 부담마저 덜어주곤 한다.</p>

<p>만약, 나에게 당장 2주뒤에 전 세계 사람들이 보는 생중계 방송에서 케이크를 정확히 이등분하라고 한다면 걱정과 피로로 앓아눕고 말 거다. 그 일이 나에게 엄청나게 부담이기 때문이다. 우리가 언제 부담을 느끼는지 생각해 본다면 아래와 같다.</p>
<ol>
  <li>나에게 주어진 업무가 내 능력 이상이라고 생각할 때</li>
  <li>나에게 영향을 주는 나의 예상과는 다른 흐름</li>
</ol>

<p>그렇다면 부담은 어떤 상황으로부터 나오는 것이 아니라 우리의 마음가짐에서 비롯됨을 알 수 있다.</p>

<p>부담을 받아들인다는 것은 어떤 의미일까?</p>

<h3 id="4">4.</h3>

<p>시간과 자원은 한정적인 상황에서 내 능력 이상의 ‘어쩔 수 없는 일’이 있음은 분명하다. 
하지만, ‘어쩔 수 없는 일’을 정의하는 과정은 여러 고민을 동반한다.</p>
<ul>
  <li>우선 도덕적 기준에서 ‘범법자의 어쩔 수 없는 일’은 인정되어선 안될 것이다. 범죄에 예외를 둔다는 것은 법의 존재 목적에 위반되기 때문이다. 이러한 예외의 존재는 기준을 흐리게 만들고 판단에 혼란을 준다.</li>
  <li>어쩔 수 없는 일이라는 핑계는 내가 마땅히 할 수 있는 일을 하지 못하도록 막는 장애물이 되기도 한다. 자신을 속이는 유혹이다.</li>
</ul>

<p>부담을 받아들인다는 건 어쩌면 어쩔 수 없는 일의 존재를 인정하는 걸지도 모른다. 내가 어떠한 마음가짐을 갖더라도 내가 할 수 없는 일은 존재함을 인정함. 아니면 부담 자체를 없애버리는 마음 가짐을 갖는 걸 수도.</p>

<h3 id="5">5.</h3>

<p>인격을 갖추게 된다면 진짜로 어쩔 수 없는 일을 구분해낼 수 있을까?</p>

<p>아니면 인격을 갖추게 된다면 이러한 일을 구분해야 할 부담을 받아들여, 굳이 구분할 필요가 없음을 알게 될까?</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Bridges&quot;]" /><category term="에세이" /></entry><entry><title type="html">장기 흐름</title><link href="http://localhost:4000/finance/%EC%9E%A5%EA%B8%B0-%ED%9D%90%EB%A6%84/" rel="alternate" type="text/html" title="장기 흐름" /><published>2025-08-21T00:00:00+09:00</published><updated>2025-08-21T00:00:00+09:00</updated><id>http://localhost:4000/finance/%EC%9E%A5%EA%B8%B0%20%ED%9D%90%EB%A6%84</id><content type="html" xml:base="http://localhost:4000/finance/%EC%9E%A5%EA%B8%B0-%ED%9D%90%EB%A6%84/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>

<h1 id="introduction">Introduction</h1>

<p>단기적인 오르내림은 운의 비중이 크지만, 장기적인 흐름은 신념을 가진 권력자들로 인해 만들어진다.</p>

<p>다수가 가진 감정, 다수가 하는 행동을 파고들었을 때 합리적인 이유가 없다면</p>

<p>다수의 눈을 가리는, 이유를 가진 권력자가 있을 가능성이 높다.</p>

<hr />
<p><br /></p>

<h1 id="가령">가령,</h1>

<p>‘Make America great again’이라는 신념을 가진 트럼프 대통령의 권력을 미루어 보아, 아래를 짐작할 수 있다.</p>

<ul>
  <li>집권기간동안 미국의 주식시장은 우상향할 것이다. 다만 금리를 낮추기 위해 잠깐의 조정이 올 수 있다.</li>
  <li>미국과 중국의 패권경쟁에 관여하는, 세계무대에서 중요한 산업(우주 진출 등이 있겠다)은 수혜를 입을 것이다.</li>
  <li>비트코인(미국의 비축자산)과 이더리움(플랫폼 코인의 압도적인 점유율)의 강세는 예견된 수순이다.</li>
</ul>

<p>이러한 큰 흐름 속에서 수익을 얻는 것이 목적인 하위 권력자는 단기적인 가격 변동을 주도하여 다수의 여론을 만들고 차익을 얻는다.</p>

<p>주식은 기업과 산업의 가치를 아는 소수가 이익을 실현하는 도구이다.</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Finance&quot;]" /><category term="에세이" /></entry><entry><title type="html">[Experience] A 서비스를 런칭하며</title><link href="http://localhost:4000/projects/AI_Report/" rel="alternate" type="text/html" title="[Experience] A 서비스를 런칭하며" /><published>2025-08-13T00:00:00+09:00</published><updated>2025-08-13T00:00:00+09:00</updated><id>http://localhost:4000/projects/AI_Report</id><content type="html" xml:base="http://localhost:4000/projects/AI_Report/"><![CDATA[<hr />

<p class="notice--info">A 서비스를 기획하고, 개발하고, 런칭한 과정에서 배우고 느낀 것에 대한 기록입니다.</p>

<h1 id="introduction">Introduction</h1>
<p>2024년 한 해 동안 서비스의 아이데이션, 구체화, 개발, 사업화까지의 전 단계를 깊숙히 경험하였다.
새로움의 연속이었고 많이 배우고 바뀌었다.</p>

<p>회고하기에 적당한 시기인 듯 하여 한해를 돌아보며 작성한다.</p>

<p>구현한 프로덕트는 배경지식을 바탕으로 DB를 만들고, LLM을 이용하여 적절한 내용을 끌어와, 입력에 맞는 아웃풋을 제공하는 서비스이다.</p>

<p>글의 요지를 불분명하게 하거나 이해충돌을 일으킬 여지가 있는 세부적인 내용은 작성하지 않았다.</p>

<h2><br /></h2>

<h1 id="1-teamwork--어떻게-협력하는가">1. Teamwork : 어떻게 협력하는가</h1>

<p>팀워크를 하며 느낀, 1+1이 2를 초과하는 데에 필요한 중요한 두 가지 요소이다.</p>

<h2 id="11-목적의식의-alignment"><em>1.1 목적의식의 Alignment</em></h2>

<p>‘왜 하는가’에 대한 공유된, 명확한 합의가 필요함을 알았다. 나와 팀원이 추구하는 가치가 다를 때, 문제 상황에서 각자의 중요한 가치에 따른 가장 좋은 해결책이 달라진다. 가치판단이 관여하는 문제는 논리적인 설득이나 합의가 어렵다. 모든 구성원이 같은 방향을 바라보아야 팀워크가 효율적이다.</p>

<h2 id="12-공고한-신뢰"><em>1.2 공고한 신뢰</em></h2>

<p>신뢰라 함은, 능력에 대한 신뢰와 사람됨에 대한 신뢰를 모두 포함한다.</p>

<ul>
  <li>
    <p>나의 능력에 대한 상대의 신뢰는 책임감을, 상대의 능력에 대한 나의 신뢰는 안정감을 만든다.</p>
  </li>
  <li>
    <p>서로의 사람됨에 대한 신뢰는, 곧 나의 실수나 의견 제시가 비난받지 않을 것이라는 심리적 안정감(Psychological Safety)로 이어져 각자의 능력을 온전히 발휘할 수 있게 만든다.</p>
  </li>
</ul>

<p>관련하여 <a href="https://www.leadingsapiens.com/project-aristotle/">구글의 Project Aristotle 연구</a>가 흥미로워 링크를 첨부한다.</p>

<h2 id="-1"><br /></h2>

<h1 id="2-product-market-fit">2. Product-Market Fit</h1>

<p>클레이튼 크리스텐슨의 <a href="https://jobs-to-be-done.com/jobs-to-be-done-a-framework-for-customer-needs-c883cbf61c90">‘Jobs-to-be-Done’ 프레임워크</a>에 따르면, 고객은 제품의 기능 자체가 아니라 그것이 해결하는 ‘일(Job)’을 위해 구매한다고 한다. 여기서 중요한 것은 이 ‘일’이 단순한 기능적 니즈를 넘어 감정적, 사회적 욕구까지 포괄한다는 점이다.</p>

<p>Product의 상업적 가치는 판매자가 아닌 구매자가 결정하며, 따라서 ‘실제로 어떤지’보다 ‘어떻게 느껴지는지’가 중요하다.</p>

<h2 id="21-세심함"><em>2.1 세심함</em></h2>

<p>사용자의 감정적 니즈를 정확히 파악하고 그 부분에서 차별화 요소를 주는 게 필요하더라.</p>

<p>더 성능 좋은 모델로, 더 깊이 있는 아웃풋을 제공하는 것보다, 오히려 세심하고 작은 요소들에서 사람들은 임펙트를 느낀다. 한눈에 들어오는 UI, 사용자의 입장을 고려한 가이드, 읽기 쉬운 수식의 렌더링 등이다. 반면 성능이 좋더라도 이러한 작은 부분들이 채워지지 않을 때 소비자가 느끼는 서비스의 품질은 크게 하락한다.</p>

<h2 id="22-human-in-loop"><em>2.2 Human in loop</em></h2>

<p>자동화와 AI가 발달할수록 ‘인간다움’의 가치는 더욱 부각된다고 생각한다. 공장에서 찍어내는 물건이 많아질수록 장인의 수제품의 가치는 높아진다. 가공식품이 다양화되지만 파인다이닝에 대한 수요는 여전하다.</p>

<p>AI를 이용한 output을 상품화하는 과정에서, AI의 hallucination와 blackbox model에 대한 불신이 꽤나 깊숙히 있음을 느꼈다.</p>

<p>‘쉽고 빠른 것, 남들과 똑같은 것’에 대한 거부감과 판매자의 진정성에 대한 니즈를 확인하며 AI가 사람을 아예 대체하는 건 역시 지금으로서는 요원하구나 생각했다.</p>

<h2 id="-2"><br /></h2>

<h1 id="3-어떠한-가치를-창출할-것인가">3. 어떠한 가치를 창출할 것인가</h1>

<h2 id="31-판매가-잘-되는-상품이-좋은-상품인가"><em>3.1 판매가 잘 되는 상품이 좋은 상품인가?</em></h2>

<p>단기 판매는 주의를 모으는 기술로도 충분히 달성된다. 강렬한 광고, 소비자의 포모(Fear of Missing Out)를 이용하는 것, 심리적인 가격 정책 등을 활용하면 유입은 늘지만 지속가능성은 부재하다. 경쟁사도 같은 전략을 사용하면 결국 전체적인 고객 획득 비용만 상승할 뿐이다.</p>

<p>그러면 유입된 고객이 유지되는, 장기적인 판매가 이루어지는 상품은 좋은 상품인가? 확실히 ‘사업적으로 좋은’ 상품에는 가깝지만 충분조건은 아니다.</p>

<p>“잘 팔리는가?”에 앞서 “무엇을 세상에 더하는가?”에 대한 명확한 답이 필요하다.</p>

<h2 id="32-파이를-넓히는-일"><em>3.2 파이를 넓히는 일</em></h2>

<p>서비스업의 상당수는 제로섬 게임의 성격을 띠고 있다. 기존 시장의 파이를 재분배하는 것에 그침을 의미한다.</p>

<p>진정한 비즈니스에서의 성공은 기존 시장에서 파이를 나눠 가지는 것이 아니라, 새로운 가치를 창출하여 전체 파이를 키우는 데서 나온다고 생각한다. 단순히 경쟁 회사로부터 고객을 빼앗아오는 차원을 넘어서, 이전에 존재하지 않았던 해결책을 제시하거나 잠재되어 있던 니즈를 찾아내어 경쟁 자체를 무의미하게 만드는 것이다.</p>

<p>돌이켜 보면 이 일을 시작한 이유도, 이 분야에서의 새로운 시도; 없는 가치를 창출하는 일이라는 생각이 들었기 때문이다. 내가 개발하는 것의 가치를 고민하는 과정은 스스로가 중요하게 여기는 가치를 고민하는 것에서부터 시작했기에 마냥 즐겁지만은 않았다. 초기의 믿음이 흔들리는 순간들도 몇 번 마주했다.
그러나 고민의 과정에서 한층 성장했음은 확실히 이야기할 수 있다.</p>

<h2 id="33-그래서-나는"><em>3.3 그래서 나는</em></h2>

<p>이번 경험을 통해 안 것은, 나는 파이를 넓히는 일에 흥미가 있다는 것과, 어떤 파이인지가 꽤나 중요하다는 것이다.</p>

<p>당분간은 아래와 같은 자질을 계속 가꾸고 싶다.</p>

<ul>
  <li>적절한 질문을 던지고 올바른 사람들과 협력할 수 있는 호기심과 열린 자세</li>
  <li>나의 모름과 한계를 마주할 수 있는 인간적 성숙함</li>
  <li>‘할 수 있는 것’에 휘둘리지 않고 ‘해야 하는 것’을 명확히 그러나 유연하게 아는 것</li>
</ul>

<p>지속 가능한 동력은 나에게서 비롯한다.</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Projects&quot;]" /><category term="Business" /><category term="AI" /></entry><entry><title type="html">Two Node Embedding Approaches - Shallow and NN-based</title><link href="http://localhost:4000/graphs/Two-Node-Embedding-Approaches/" rel="alternate" type="text/html" title="Two Node Embedding Approaches - Shallow and NN-based" /><published>2025-08-11T00:00:00+09:00</published><updated>2025-08-11T00:00:00+09:00</updated><id>http://localhost:4000/graphs/Two%20Node%20Embedding%20Approaches</id><content type="html" xml:base="http://localhost:4000/graphs/Two-Node-Embedding-Approaches/"><![CDATA[<hr />

<h1 id="introduction">Introduction</h1>

<p><a href="https://darejinn.github.io/graphs/Representation-Learning-on-Graphs/">앞선 글</a> 에서 설명했듯, Graph Representation Learning은 그래프의 요소(node, edge, subgraph)를 저차원 벡터로 매핑하는 method이다. 많은 survey가 여러 요소 중에서도 <strong>node embedding</strong>에 초점을 맞추는 이유는 다음과 같다.</p>

<ol>
  <li>
    <p><strong>Edge/subgraph embedding이 node embedding의 후처리로 귀결</strong>되는 경우가 많다.</p>

    <p>예를 들어</p>
    <ul>
      <li>edge $(v_i,v_j)\in E$에 대해 node embedding $\mathbf{z}_i,\mathbf{z}_j$가 주어지면 hadamard/mean/weighted-L1·L2 같은 이항 연산으로 아래처럼 edge embedding을 만든다.</li>
    </ul>

\[\mathbf{z}_{(i,j)}=\mathbf{z}_i\odot\mathbf{z}_j\]

    <ul>
      <li>subgraph $\mathcal{G}[\mathcal{S}]$의 embedding도 보통 $\mathcal{S}\subset\mathcal{V}$에 포함된 node embedding을 집계(average/attention 등)해서 아래와 같이 얻는다.</li>
    </ul>

\[\mathbf{z}_{\text{subgraph}}=\frac{1}{|\mathcal{S}|}\sum_{v_i\in\mathcal{S}}\mathbf{z}_i\]
  </li>
  <li>
    <p>Edge/Subgraph embedding을 직접 학습하더라도 절차는 <strong>node embedding 학습과 본질적으로 유사</strong>하다.</p>
    <ul>
      <li>예를 들어 타깃 subgraph의 모든 node에 연결된 dummy node를 추가하고 그 node의 embedding을 학습 대상으로 두면 사실상 node embedding 학습과 거의 동일한 형태가 된다.</li>
    </ul>
  </li>
</ol>

<p>따라서 node embedding을 이해하면 edge/subgraph embedding도 같은 틀 안에서 상당 부분 설명된다. 이하에서는 <strong>encoder를 어떻게 정의하는지</strong>에 따라 node embedding을 <strong>shallow embedding</strong>과 <strong>NN-based embedding</strong>으로 비교해 정리한다. 본 글은 <a href="https://arxiv.org/abs/1709.05584">“<em>Representation Learning on Graphs</em>” (2017) </a>에서 제시한 개념을 바탕으로 필자의 해석을 더해 작성하였다.</p>

<hr />

<h2 id="review--learning-framework"><em>Review : Learning framework</em></h2>

<p><em>‘그래프에서 가까운 node는 embedding 공간에서도 가깝다’</em>는 <strong>structural assumption</strong> 아래, 다음 세 가지를 설계한다.</p>

<ul>
  <li>
    <p><strong>그래프 상의 유사도 $s_G$</strong></p>

\[s_G:\mathcal{V}\times\mathcal{V}\to\mathbb{R}^+,\qquad
s_G(i,j)\in\{A_{ij},\ k\text{-hop},\ \text{랜덤워크 공출현확률}\}\]
  </li>
  <li>
    <p><strong>Embedding 공간의 decoder $\mathrm{DEC}$</strong></p>

\[\mathrm{DEC}:\mathbb{R}^d\times\mathbb{R}^d\to\mathbb{R},\quad
\text{예: }\ \mathbf{z}_i^\top\mathbf{z}_j,\ -\lVert\mathbf{z}_i-\mathbf{z}_j\rVert_2^2,\ \sigma(\mathbf{z}_i^\top\mathbf{z}_j)\]
  </li>
  <li>
    <p><strong>Encoder $\mathrm{ENC}$와 loss function(learning objective)</strong></p>
  </li>
</ul>

\[\mathbf{z}_i=\mathrm{ENC}(v_i),\qquad
  \mathcal{L}=\sum_{(i,j)\in\mathcal{D}}\ell\!\Big(\mathrm{DEC}(\mathbf{z}_i,\mathbf{z}_j),\ s_G(i,j)\Big)\]

<p>여기서 <strong>structural assumption은 ‘무엇을 가깝게 유지할지’에 대한 inductive bias,</strong> 즉 <strong>네트워크 구조에 관한 모델의 prior</strong>다.</p>

<p>$s_G$를 어떻게 정의할지(무엇을 ‘유사’로 볼지), $\mathrm{DEC}$를 어떻게 둘지(embedding에서 유사를 어떻게 수치화할지), $\mathrm{ENC}$를 어떻게 설계할지(그 유사를 재현하도록 표현을 만들지)에 이 가정이 명시적·암묵적으로 들어 있다.</p>

<hr />
<p><br />
<br /></p>

<h1 id="two-node-embedding-approaches">Two Node Embedding Approaches</h1>
<hr />
<p>위와 같은 $\mathrm{ENC}$–$s_G$–$\mathrm{DEC}$ 프레임워크 안에서, <strong>학습이 일어나는 층위</strong>에 따라 접근을 두 가지로 나눌 수 있다.</p>

<ul>
  <li><strong>Shallow</strong> : 미리 정한 $s_G$에 맞춰 <strong>node별 embedding 자체</strong>를 <strong>직접</strong> 최적화한다.</li>
  <li><strong>NN-based</strong>: <strong>유사도를 만들어내는 연산</strong>(autoencoder, message passing)를 학습해 embedding을 <strong>간접</strong>적으로 만든다. 파라미터는 <strong>node 간 공유</strong>된다.</li>
</ul>

<hr />

<h2 id="1-shallow-embedding"><em>1. Shallow embedding</em></h2>

<p>Shallow embedding은 <strong>node ID를 임베딩 행렬의 열(column)에 직접 매핑</strong>하는 방식이다.</p>

\[Z \in \mathbb{R}^{d \times |\mathcal{V}|}, \qquad \mathrm{ENC}(v_i) = Z_{\cdot i} = \mathbf{z}_i\]

<p>Encoder 구조가 단순하기 때문에, 무엇을 보존할지에 대한 선택이 거의 전적으로 <strong>그래프 상의 유사도 정의 $s_G$와 Decoder</strong> 설계에 담긴다.</p>

<hr />

<h3 id="1-1-factorization-based-approaches"><em>1-1. Factorization-based approaches</em></h3>

<ul>
  <li>
    <p><strong>무엇을 보존할지 ($s_G$ 설정)</strong><br />
유사도 행렬 $S$는 인접성, 근접성, 전파 도달성 등 다양한 그래프 속성을 반영하도록 정의할 수 있다. 예시는 다음과 같다.</p>

\[S = A \quad \text{(adjacency)}, \qquad
S = A^k \quad (k\text{-hop}),
\qquad
S = \text{Katz}, \ \text{RPR}, \ \text{Jaccard} \ \text{등}\]

    <p>$S$의 정의에 따라, embedding이 강조하는 구조적 특성(커뮤니티, 국소 근접성, 전파 가능성 등)이 달라진다.</p>
  </li>
  <li>
    <p><strong>Embedding에서 어떻게 읽을지 (Decoder)</strong></p>

    <p>대표적인 Decoder 정의는 다음과 같다.</p>

\[\mathrm{DEC}(\mathbf{z}_i, \mathbf{z}_j) = \mathbf{z}_i^\top \mathbf{z}_j
\quad \text{또는} \quad- \lVert \mathbf{z}_i - \mathbf{z}_j \rVert_2^2\]
  </li>
  <li>
    <p><strong>Loss function (learning objective)</strong></p>

    <ul>
      <li>
        <p>Decoder가 내적(inner product)일 경우
\(\min_{Z} \ \lVert S - Z^\top Z \rVert_F^2\)</p>
      </li>
      <li>
        <p>Decoder가 음의 유클리드 거리일 경우
\(\sum_{i,j} W_{ij} \lVert \mathbf{z}_i - \mathbf{z}_j \rVert_2^2 
\ = \ 2\, \mathrm{tr}(Z L Z^\top)\)</p>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="1-2-random-walk-based-approaches"><em>1-2. Random walk-based approaches</em></h3>

<ul>
  <li>
    <p><strong>무엇을 보존할지 ($s_G$ 설정)</strong></p>

    <p>랜덤 워크를 통해 생성한 노드 시퀀스에서 <strong>공출현(co-occurrence) 강도 또는 확률</strong>을 노드 간 유사도로 정의한다.</p>
    <ul>
      <li><strong>DeepWalk</strong>: 편향 없는(unbiased) 랜덤 워크를 사용.</li>
      <li><strong>node2vec</strong>: 하이퍼파라미터 $p, q$로 BFS/DFS 성향을 조절하여, 국소적 조밀성(locality)과 구조적 역할(role) 간의 균형을 조정.</li>
    </ul>
  </li>
  <li>
    <p><strong>Embedding에서 어떻게 읽을지 (Decoder)</strong></p>

    <p>Factorization 기반 방법과 유사하게, 내적 기반 Decoder를 사용하되, <strong>유사도 행렬 S</strong>가 아니라 <strong>랜덤 워크 확률</strong>을 근사하도록 학습한다.<br />
즉, 다음 조건이 만족되도록 embedding을 학습한다.</p>

\[\mathrm{DEC}(\mathbf{z}_i, \mathbf{z}_j)=
\frac{\exp(\mathbf{z}_i^\top \mathbf{z}_j)}
     {\sum_{v_k \in \mathcal{V}} \exp(\mathbf{z}_i^\top \mathbf{z}_k)}
\ \approx \ p_{G,T}(v_j \mid v_i)\]

    <p>여기서 $p_{G,T}(v_j \mid v_i)$는 그래프 $G$에서 노드 $v_i$에서 시작해 길이 $T$의 랜덤 워크를 수행할 때 노드 $v_j$를 방문할 확률이며, 일반적으로 $T \in { 2, \dots, 10 }$ 범위로 설정된다.</p>
  </li>
  <li>
    <p><strong>Loss function (learning objective)</strong></p>

    <p>위 확률 근사를 위해 다음 <strong>cross enthropy loss</strong>을 최소화한다.</p>

\[\mathcal{L} = \sum_{(v_i, v_j) \in D} - \log \left( \mathrm{DEC}(\mathbf{z}_i, \mathbf{z}_j) \right)\]

    <p>여기서 $D$는 각 노드 $v_i$에서 시작하는 랜덤 워크로부터 샘플링된 $(v_i, v_j)$ 쌍들의 집합이다.
그러나 위 식의 분모 계산은 $O(|\mathcal{V}|)$ 시간이 소요되므로, 전체 학습 비용은 $O(|D||\mathcal{V}|)$로 매우 크다. 이를 해결하기 위해:</p>

    <ul>
      <li><strong>DeepWalk</strong>의 경우 : <strong>Hierarchical softmax</strong>를 사용하여, 이진 트리 구조로 정규화 항 계산을 가속한다.</li>
      <li><strong>Node2vec</strong>의 경우: <strong>Negative sampling</strong> 사용하여, 전체 노드 집합 대신 무작위로 선택한 소수의 negative sample로 분모를 근사한다.</li>
    </ul>
  </li>
  <li>
    <p>가장 대표적인 random walk based embedding method를 다루었는데, Deepwalk와 node2vec 외에도 LINE, HARP 등의 다양한 방법이 있다.</p>
  </li>
</ul>

<hr />
<p><br /></p>

<h2 id="2-nn-based-embedding"><em>2. NN-based embedding</em></h2>

<p>Shallow가 node-id만 입력으로 받아 바로 embedding을 lookup하는 것과 달리, NN-based는 <strong>node 특징 $\mathbf{x}_i$와 이웃 $\mathcal{N}(i)$를 입력으로 받아 처리하는 네트워크</strong>를 학습한다.</p>

<h3 id="2-1-neighborhood-autoencoder-dngr-sdne"><em>2-1. Neighborhood Autoencoder (DNGR, SDNE)</em></h3>

<p>각 노드 $i$마다, <strong>이웃 분포(neighborhood distribution)</strong> 를 나타내는 벡터  $\mathbf{s}_i \in \mathbb{R}^{|\mathcal{V}|}$를 생성한 뒤, 이를 <strong>Autoencoder</strong>로 복원한다.
여기서 $\mathbf{s}_i$는 행렬 $S$의 $i$번째 행에 해당하며, $S$는 노드 간 유사도 $s_G(v_i, v_j)$를 담고 있다.<br />
즉, $\mathbf{s}_i$는 노드 $v_i$가 그래프 내 모든 다른 노드와 가지는 유사도를 포함하는 고차원의 벡터다.</p>

<p><strong>Learning objective</strong>는 다음과 같다.</p>

\[\min_{\theta,\phi} \ \sum_i 
\left\lVert
\mathbf{s}_i - \mathrm{DEC}_\phi\big(\mathrm{ENC}_\theta(\mathbf{s}_i)\big)
\right\rVert_2^2\]

<p><br />
Shallow embedding과 달리 <strong>파라미터가 노드 간 공유</strong>되어 연산이 효율적이다. 하지만 입력 차원이 $|\mathcal{V}|$에 고정되어 있어 대규모 그래프 적용이 어려우며, 새로운 그래프에 대한 적용도 제한적이다. ( <strong>transductive</strong> )</p>

<h3 id="2-2-neighborhood-aggregation-gcngraphsagegat"><em>2-2. Neighborhood Aggregation (GCN/GraphSAGE/GAT)</em></h3>

<p><strong>Message Passing</strong>으로 이웃 표현을 aggregate해 자신의 표현과 결합하는 연산을 여러 층 반복한다.</p>

<blockquote>

  <p>참고로, <strong>Message Passing</strong>은 “Neural message passing for quantum chemistry.”논문에서 처음 GNN의 framework를 통합적으로 설명하기 위해 제시한 개념으로, 이후 많은 논문들에서 해당 개념을 차용하여 GNN을 설명하기 시작하였다. 
딥마인드의 Petar Veličković는 최신의 ‘beyond message passing’으로 여겨졌던 GNN 기법도 본질적으로 message passing framework으로 모두 설명할 수 있다고  <a href="https://arxiv.org/abs/2202.11097">“Message passing all the way up” </a> 에서 주장하였다.</p>

</blockquote>

<p><strong>연산 과정은 다음과 같다.</strong></p>

\[\mathbf{h}_i^{(0)} = \mathbf{x}_i, \quad
\mathbf{h}_i^{(k)} = \mathrm{COMBINE}^{(k)}\Big(
  \mathbf{h}_i^{(k-1)}, \
  \mathrm{AGG}^{(k)}\{\mathbf{h}_j^{(k-1)} : j \in \mathcal{N}(i)\}
\Big), \quad
\mathbf{z}_i = \mathbf{h}_i^{(K)}\]

<ol>
  <li>초기 임베딩은 입력 특징 $\mathbf{x}_i$로 설정한다.</li>
  <li>각 단계에서 이웃 노드 임베딩을 집계($\mathrm{AGG}$)한다.</li>
  <li>집계된 이웃 표현과 자신의 이전 단계 표현을 결합($\mathrm{COMBINE}$)한다.</li>
  <li>이 과정을 $K$번 반복하여 최종 임베딩 $\mathbf{z}_i$를 얻는다.</li>
</ol>

<p>위 encoder 연산에서 COMBINE function과 AGG function을 어떻게 정의하는지에 따라 GNN의 세부 방법론(GCN, GraphSAGE, GAT 등) 들이 나누어진다.</p>

<p><br /></p>

<p><strong>GNN은 구조에 대한 연산(structural assumption)을 $\mathrm{ENC}$가 담당하므로, $\mathrm{DEC}$와  loss function(learning objective) 은 태스크에 맞춰 유연하게 설정할 수 있다.</strong></p>
<ul>
  <li>링크 예측 : $\mathrm{DEC}(\mathbf{z}_i,\mathbf{z}_j)=\sigma(\mathbf{z}_i^\top\mathbf{z}_j)$, negative sampling</li>
  <li>node 분류 : $\hat{\mathbf{y}}_i=\mathrm{softmax}(W\mathbf{z}_i)$, cross-enthropy</li>
  <li>그래프/subgraph 관련 supervised task: $\mathrm{READOUT}$으로 node embedding을 post processing한 후 태스크별 $\mathrm{DEC}$ 사용</li>
</ul>

<p><br /></p>

<p>이 방법은 가장 진보한, 현재에도 활발히 연구되고 있는 graph representation 방식으로, 흔히 GNN을 이야기하면 보통 neighborhood aggregation을 바탕으로 한 이 message passing을 상정한다.</p>

<p>Neighborhood Autoencoder 방법과 마찬가지로 <strong>파라미터 공유</strong>가 이루어지며, 순차적으로 주변 정보를 aggregate하므로 모델 크기가 ‘그래프 크기와 관계 없이’ 효율적으로 작게 유지된다. <strong>Node feature/edge weight</strong> 등 그래프 관련 meta information을 자연스럽게 활용할 수 있으며, 처음 보는 그래프에도 적용이 수월하다. ( <strong>inductive</strong> )</p>

<hr />

<h2 id="마무리하며">마무리하며</h2>

<ul>
  <li>
    <p>Graph representation learning의 framework는,
<strong>(1) 무엇을 유사로 볼지($s_G$)</strong>, <strong>(2) embedding에서 그 유사를 어떻게 읽어낼지($\mathrm{DEC}$)</strong>, <strong>(3) 그 유사를 재현하도록 어떤 연산을 학습할지($\mathrm{ENC}$)</strong> 를 설정(<strong>structural assumption</strong>) 하는 것이 핵심이다.</p>

    <ul>
      <li><strong>Shallow</strong>는 <strong>(1)</strong> 유사도를 정의하고(S 혹은 공출현확률) 그에 맞춰 <strong>(2)</strong> embedding 열의 내적이 비슷하도록 <strong>(3)</strong> embedding 열을 각 node id마다 따로따로 lookup한다.</li>
      <li><strong>Neighborhood Autoencoder</strong>는 <strong>(1)</strong> node마다 이웃 node의 정보를 나타내는 $\mathbf{s}_i$를 정의해, <strong>(3)</strong> 오토인코더를 이용해 <strong>(2)</strong> $\mathbf{s}_i$를 복원하도록 효율적으로 학습한다.</li>
      <li><strong>Neighborhood Aggregation(GNN)</strong> 은 <strong>(1)(3)</strong> “무엇을 보존할지”가 <strong>message passing network</strong>에 녹아 있으며, 태스크에 따라 <strong>(2)</strong> $\mathrm{DEC}$/loss를 유연하게 붙일 수 있다.</li>
    </ul>
  </li>
</ul>

<p>구체적 방법론과 수식 유도는 후속 글에서 더 다룰 예정이다.</p>

<h3 id="첨언">첨언</h3>

<ol>
  <li>두 approach로 node embedding method를 나누어 설명하였지만, 두 approach는 배타적이지 않으며 해당하지 않는 예외도 존재한다. 가령 random walk 기반 sampling을 한 뒤 neural network를 학습시키는 방법도 있다. 또한 message passing의 fundamental한 한계를 지적하며 아예 새로운 framework를 제안하는 연구들도 있다. <a href="https://arxiv.org/pdf/2501.18739"><em>(ex. 랜덤워크를 통해 얻어낸 pattetn을 transformer에 넣는 방법을 제안한 최신 논문이다</em>)</a></li>
  <li><a href="https://arxiv.org/abs/1709.05584">“<em>Representation Learning on Graphs</em>” (2017) </a>의 내용을 주로 참고한 이유는, 교신저자께서 graph machine learning의 대가이신, CS224W의 레즈코벡 교수님이시며, 특정 태스크(community detection, link prediction)나 메소드에 치중하지 않고 traditional method부터 GNN까지의 전반적인 개념을 한 프레임워크로 설명하시었기 때문이다. (<em>서울대학교 컴퓨터공학부 김선 교수님께서도 그래프 마이닝 강의 중, 해당 survey 논문을 교본으로 사용하셨다.</em>)</li>
  <li>본 글에서는 시간축에 따라 변화하는 dynamic graph는 다루지 않았다.</li>
  <li>학습이 일어나는 순방향 framework에 초점을 맞추었지만, 사실 최적화 방법도 매우 다양하다. 가령 라플라시안 행렬 분해는 목적함수를 최소화하는 문제를 일반화된 고윳값 문제의 해를 찾는 방법으로 해결한다. 신경망 기반 GRL은 일반적으로 gradient descent로 loss를 줄이는 방향으로 parameter를 update한다.</li>
</ol>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Graphs&quot;]" /><category term="Graphs" /><category term="Representation learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">우연 속의 필연</title><link href="http://localhost:4000/thoughts/08.-%EC%9A%B0%EC%97%B0-%EC%86%8D%EC%9D%98-%ED%95%84%EC%97%B0/" rel="alternate" type="text/html" title="우연 속의 필연" /><published>2025-08-06T00:00:00+09:00</published><updated>2025-08-06T00:00:00+09:00</updated><id>http://localhost:4000/thoughts/08.%20%EC%9A%B0%EC%97%B0%20%EC%86%8D%EC%9D%98%20%ED%95%84%EC%97%B0</id><content type="html" xml:base="http://localhost:4000/thoughts/08.-%EC%9A%B0%EC%97%B0-%EC%86%8D%EC%9D%98-%ED%95%84%EC%97%B0/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>

<h1 id="introduction">Introduction</h1>
<hr />
<p>세상은 대체로 모호하다.</p>

<ul>
  <li>정책은 정의로움과 부당함의 속성을 동시에 가진다.</li>
  <li>객체는 상태의 중첩에 놓여 있으며 온전히 입자이지도, 온전히 파동이지도 않다.</li>
  <li>가설 검증에서 채택되는 가설은 언제나 오류를 범할 확률을 동반한다.</li>
</ul>

<p>세상은 흑과 백이 아닌 ‘덜 회색’과 ‘더 회색’에 가깝고, 벌어지는 일들은 예외 없는 인과가 아닌 상관성과 확률에 기반한다.</p>

<p>완전무결함은 사람이 쌓아 올린 믿음이 영역이 아닌 실재의 영역에서는 찾아보기 어렵다.</p>

<p><br /></p>

<h1 id="불확실함을-인정하며-불안하지-않을-수-있는가">불확실함을 인정하며 불안하지 않을 수 있는가</h1>
<hr />
<p>하지만 우리는 대체로 완전무결함을 추구한다. 0과 1의 명확한 구분에서 안정감을 느끼며 확실한 미래를 예측하고 싶어한다. 끊임없이 선택해야 하는 삶의 과정을 고려할 때 자연스러운 속성이다.</p>

<p>그러나 불확실함을 부정하며 완전무결함을 추구하는 것은 종종 부작용을 낳는다. 부정하는 과정에서 무시한 - 주로 크기가 작은 - 속성들이 사실은 중요했거나, 시간이 지남에 따라 중요해지기도 하기 때문이다.</p>

<p>불확실함을 부정하는 믿음은 독단으로 변모하기 쉽다.</p>

<p><br /></p>

<h1 id="행복한-시지푸스">행복한 시지푸스</h1>
<hr />
<p>철학자 알베르 카뮈는 돌이 굴러 떨어질 것을 앎에도 끊임없이 돌을 밀어올리며 삶의 의미를 찾는 ‘행복한 시지푸스’를 이야기한다.</p>

<p>카뮈가 제시하는 삶의 태도는 다음 두 가지와는 구별된다:</p>

<ol>
  <li>부정적 낙관주의: 돌이 떨어지는 것을 부정하며 행복하게 돌을 밀어올리는 삶</li>
  <li>허무주의: 돌이 떨어지는 것을 알고 아예 돌을 밀어올리지 않는 삶</li>
</ol>

<p>카뮈는 이 둘이 아닌, 돌이 떨어지는 것을 앎에도 행복하게 돌을 밀어올리는 삶을 살아야 한다고 말한다.</p>

<p>삶의 이유를 한참 고민하던 예과 2학년 시기, 나는 시지푸스의 행복이 이해가 되지 않았었다. 어떻게 행복할 수 있지 싶었다.</p>

<p><br /></p>

<h1 id="우연-속의-필연">우연 속의 필연</h1>
<hr />
<p>그러나 생각해보면, 사실 우리는 누구나 시지푸스와 같은 처지이다.</p>

<p>우리는 스스로가 어떤 우연으로 지금 당장, 혹은 언제든 삶을 마감할 수 있음을 안다. 관계가 영원하지 않을 수 있음도 안다. 하지만 몇 시간 뒤에, 혹은 내일 당장 기막힌 확률로 세상을 떠날 것을 걱정하며 사는 사람은 거의 없다.
우리는 불안함 없이 죽음의 위험을 유보하며 삶에 충실하게 살아간다. 영원을 약속하고 사랑하며 살아간다.</p>

<hr />

<p>이러한 태도는 다른 차원의 불확실함에도 적용될 수 있다고 생각한다.</p>

<p>내가 꿈꾸는 미래가 불친절한 변수로 인해 이루어지지 않을 수 있음을 알면서도, 내가 틀릴 수 있는 가능성을 인정하면서도,</p>

<p>불확실함을 부정하지도, 그것에 압도되지도 않으면서, 현재의 선택과 행동에 온전히 투신하는 삶을 살고 싶다.</p>

<p>우연을 앎에도 필연을 믿는 삶을 살고 싶다.</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Thoughts&quot;]" /><category term="에세이" /></entry><entry><title type="html">믿음의 영역</title><link href="http://localhost:4000/thoughts/07.-%EB%AF%BF%EC%9D%8C%EC%9D%98-%EC%98%81%EC%97%AD/" rel="alternate" type="text/html" title="믿음의 영역" /><published>2025-08-03T00:00:00+09:00</published><updated>2025-08-03T00:00:00+09:00</updated><id>http://localhost:4000/thoughts/07.%20%EB%AF%BF%EC%9D%8C%EC%9D%98%20%EC%98%81%EC%97%AD</id><content type="html" xml:base="http://localhost:4000/thoughts/07.-%EB%AF%BF%EC%9D%8C%EC%9D%98-%EC%98%81%EC%97%AD/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>
<p><br /></p>
<blockquote>
  <p>지식이 믿음에 대해서 우위를 보이는 요소인 객관적 사실성과 보편성은 판단과 결정을 통한 행위의 영향력과 관련해서 의외로 짧은 유효거리를 가질 뿐이다. 다소간의 확실한 요소들이 유입될 뿐인 미래의 전개는 언제나 우연성과 불확실성을 포함한다. 반면에 믿음은 다양한 삶의 연관 안에서 형성된 개인적 출발점의 확신을 통해 지식이 달성할 수 없는 총체적인 실재 전체와의 연관을 실현해 내고 인간을 고무하고 희망을 주는 힘이다.</p>

  <p>양대종. “지식과 믿음의 연관에 대한 고찰.” 철학연구 (2023): 119-145. 중.</p>
</blockquote>

<p><br />
삶이 거시적으로 좋은 방향으로 흘러가고 있다는 믿음은, 예측하지 못한 변수를 회피하지 않고 마주할 수 있게 만들며</p>

<p>노력이 어떤 순간에 내게 돌아올 것이라는 믿음은, 결과의 불확실함에 흔들리지 않고 최선을 다할 수 있게 만든다.</p>

<p>하고 있는 일이 옳고 선하다는 믿음은, 누군가의 비난에 상처받지 않을 수 있는 힘을 주며</p>

<p>타인의 선의와 사랑에 대한 믿음은, 함께 사는 세상에서 행복의 근원에 가깝다고 생각한다.</p>

<p><br />
<br />
‘그냥’이라는 이유는 비논리적이지만 역설적으로 그러해서 반박불가하며 강력하다.</p>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Thoughts&quot;]" /><category term="에세이" /></entry><entry><title type="html">투자의 목적</title><link href="http://localhost:4000/finance/%ED%88%AC%EC%9E%90%EC%9D%98-%EB%AA%A9%EC%A0%81/" rel="alternate" type="text/html" title="투자의 목적" /><published>2025-08-02T00:00:00+09:00</published><updated>2025-08-02T00:00:00+09:00</updated><id>http://localhost:4000/finance/%ED%88%AC%EC%9E%90%EC%9D%98%20%EB%AA%A9%EC%A0%81</id><content type="html" xml:base="http://localhost:4000/finance/%ED%88%AC%EC%9E%90%EC%9D%98-%EB%AA%A9%EC%A0%81/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>

<h1 id="introduction">Introduction</h1>
<hr />
<p>자산을 갖고 있는 한, 투자는 필연적이다. 아무 것도 하지 않고 예금에 나의 돈을 넣어둔다면, 이 또한 한국의 원화라는 가치에 돈을 투자하고 있는 셈이다. 투자에 대한 고민의 과정을 기록한다.
<br />
<br /></p>

<h1 id="1-투자의-목적">1. 투자의 목적</h1>
<hr />
<p>무엇이든, 잘하기 위해서는 노력과 시간이 필요하다.</p>

<p>투자 역시 마찬가지이다.</p>

<p>단기투자를 잘하기 위해서는 미시적인 사람의 심리와 단기적인 이벤트, 차트의 흐름을 공부하고 살펴야 한다.</p>

<p>장기투자를 잘하기 위해서는 거시적인 세상의 흐름을 예민하게 살펴야 한다.</p>

<p>인생은 짧고 젊음은 더 짧기에, 지금 시기의 나의 노력과 시간을 어디에 얼마나 배분해야 할지 고민할 필요가 있었고</p>

<p>고민의 결론은, 20대 초반에서 중반을 향하고 있는 지금 시점에는, 세상의 가치를 살피기보다 나의 가치를 키우는 데에 집중하고 싶다는 것이었다.</p>

<p><strong>그래서 지금 나의 투자의 목적은, 최소한의 노력을 들여 적어도 나의 가치를 키우는 것에 방해되지 않는 선에서 자산을 운용하는 데에 있다.</strong></p>

<p>조금 더 구체적으로 말하면, 공부하는 와중 먹고 싶은 것을 먹고, 가끔 놀러가고 싶을 때 갈 수 있고, 소중한 주변 사람들을 챙길 수 있을 정도로 현명하게 자산을 관리하는 것이 목표이다.</p>

<p>타인이 실현할 가치를 예측하고 투자하여 10억을 만드는 것보다,</p>

<p>나의 잠재력을 키워, 내가 실현하는 가치에 투자자가 10억을 아깝지 않게 지불하게 만드는 것이</p>

<p>적어도 내 인생에 있어서는 내가 나아가고 싶은 방향이다.</p>

<p><br />
<br /></p>

<h1 id="2-어떻게-투자할-것인가">2. 어떻게 투자할 것인가</h1>
<hr />
<p>그래서 가만히 있겠다는 것이냐, 그럴 수는 없다.</p>

<p>권력자가 돈을 찍어낼 수 있는 사회구조 상 현금은 가치 하락이 예견된 자산이다. 투자를 달리기에 비유한다면, 속도에서의 완급조절은 자유이지만 올바른 방향 설정은 필요하다고 생각한다.</p>

<p>아래는 내가 고민한 어떻게 투자할 것인가에 대한 나의 방향성이다.</p>

<ol>
  <li>자산의 가치는 사람들의 기대를 반영한다. ‘사람들이 곧 기대할, 그러나 지금은 저평가된’ 가치 있는 기업을 찾는다.
  그러나 기업을 찾는 데에 있어서 거인의 어깨에 올라서는 것을 주저하지 않는다.</li>
  <li>1.에서 찾은 후보에 대해서 기술적 분석으로 매수/매도 시점을 정한다.
그러나 정확한 타이밍을 예측하려 하기보다, 추세를 확인하고 분할매매한다.
구체적으로, 상승추세의 눌림목에서 매수하고, 하락추세에 진입했을 때 매도한다.</li>
  <li>포트폴리오를 관리하는 데에 시간적, 감정적 소모를 최소화한다.
    <ul>
      <li>장이 열리는 시각에 오르고 내리는 차트를 들여다보지 않는다. 후보 기업과 주가의 추세를 가시화하여, 정한 시점에 확인한다.</li>
      <li>차트의 오르내림에 감정을 소모하지 않고 정한 규칙에 따라 매수, 매도한다.</li>
    </ul>
  </li>
</ol>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Finance&quot;]" /><category term="에세이" /></entry><entry><title type="html">매수 매도 규칙</title><link href="http://localhost:4000/finance/%EB%A7%A4%EC%88%98%EB%A7%A4%EB%8F%84%EA%B7%9C%EC%B9%99/" rel="alternate" type="text/html" title="매수 매도 규칙" /><published>2025-07-31T00:00:00+09:00</published><updated>2025-07-31T00:00:00+09:00</updated><id>http://localhost:4000/finance/%EB%A7%A4%EC%88%98%EB%A7%A4%EB%8F%84%EA%B7%9C%EC%B9%99</id><content type="html" xml:base="http://localhost:4000/finance/%EB%A7%A4%EC%88%98%EB%A7%A4%EB%8F%84%EA%B7%9C%EC%B9%99/"><![CDATA[<hr />

<p class="notice--info">원칙은 바뀔 수 있음이 원칙이다.</p>

<h1 id="introduction">Introduction</h1>

<p>성장하는 자본주의 시스템에서 통화량의 상승과 통화가치의 하락은 자연스러운 수순이다. 따라서 현금을 갖고 있기보다 현물 자산으로 바꾸어 갖고 있는 것이 현명한데, 기업의 가치를 반영하는 주식에 투자하는 방식은 그 중 하나이다.</p>

<p>그러나 파는 사람과 사는 사람이 항상 같이 존재하는 주식시장에서, 누군가가 수익을 얻는다면 동시에 누군가는 손실을 입기 마련이고, 기업 가치에 대한 정보의 열위에 있는 일반투자자는 후자에 속할 확률이 높다. 특히 이익을 확정짓고 싶어하고 손실을 기피하는 사람의 심리는 예측 가능하기에 감정적인 일반투자자는 더욱 세력의 이익실현에 휘둘리기 쉽다. 매수와 매도의 기준이 명확해야 하는 이유이다.</p>

<p>조금 더 개인적인 이야기를 해보자면, 나에게는 지금 순간의 나의 행복이 꽤 중요한 가치이다. 차트의 단기적 오르내림에 나의 행복도가 흔들리지 않기 위해서는 ‘내가 맞게 투자하고 있다.’는 믿음이 있어야 하더라. 믿음의 영역에 정답은 없고 죽기 전까지 주식이 떨어지기만 하더라도 언젠간 오른다는 믿음이 있다면 행복하기야 하겠지만… 그래도 이왕이면 믿는대로 이루어지면 더 좋으니 시장의 원리를 바탕으로 나의 매수, 매도 규칙을 정하고자 한다.
<br />
<br /></p>

<h1 id="1-매수">1. 매수</h1>
<hr />
<ul>
  <li>대상
    <ol>
      <li>내가 관심이 있고, 장기적인 상승의 근거가 명확한 분야의 회사</li>
      <li>단기적인 이벤트로 주가 상승이 예측되는 회사 (다만 높은 비중으로 투자하지는 말자)
        <ul>
          <li>임상 승인</li>
          <li>관세 / 정책의 변화</li>
          <li>단기적이고 비합리적인 하락 (다시 오를 것이 확실할 때)</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>방식</p>

    <p>상승 추세의 눌림목일 때 구매, 혹은 높은 밸류의 회사가 낮은 주가에서 횡보/반등할 때 구매</p>
    <ol>
      <li>상승 추세는 binary wave 7개 지표로 기계적으로 판단</li>
      <li>최근 급격한 상승이 이미 이루어졌다면, 관심을 갖되 당장 매입하지는 말자</li>
    </ol>
  </li>
</ul>

<p><br /></p>
<h1 id="2매도">2.매도</h1>
<hr />
<ul>
  <li>대상
    <ol>
      <li>상승추세가 깨진 경우</li>
      <li>급격한 상승으로 조정이 예상될 때 혹은 너무 매력적인 다른 종목이 생겼을 때</li>
    </ol>
  </li>
  <li>방식
    <ol>
      <li>7개 지표 중 2개 이상 깨졌을 경우 : 전량 분할 매도</li>
      <li>7개 지표 중 1개가 깨졌을 경우 : 절반 분할 매도</li>
    </ol>
  </li>
</ul>

<p><br /></p>
<h1 id="3분배">3.분배</h1>
<hr />
<ul>
  <li>현금은 10% 보유 (언젠간 현금 대신 코인을 보유하는 날이 올 것 같다)</li>
  <li>안정적인 기업 / 도전적인 기업에 분산투자하고자 한다. 투자에 조금 더 경험과 확신이 생기면, 가치가 확실한 기업에 집중투자하는 방식으로 바꾸어나가야겠다.</li>
</ul>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Finance&quot;]" /><category term="에세이" /></entry><entry><title type="html">Oveview on Graph Representation Learning</title><link href="http://localhost:4000/graphs/Representation-Learning-on-Graphs/" rel="alternate" type="text/html" title="Oveview on Graph Representation Learning" /><published>2025-07-30T00:00:00+09:00</published><updated>2025-07-30T00:00:00+09:00</updated><id>http://localhost:4000/graphs/Representation%20Learning%20on%20Graphs</id><content type="html" xml:base="http://localhost:4000/graphs/Representation-Learning-on-Graphs/"><![CDATA[<hr />

<h1 id="introduction">Introduction</h1>
<hr />
<p align="center">
  <img src="https://github.com/user-attachments/assets/022b334c-5d69-4f4f-b3fe-4935350a043f" width="600" />
</p>
<p align="center">  <em>Graph(왼쪽)과 Euclidean structured data(오른쪽) (출처: <a href="https://web.stanford.edu/class/cs224w/">CS224w)</a></em>
</p>
<p>이미지, 문장과 같이 2D grid나 1D sequence로 성분의 순서와 구조가 정형화된 euclidean structured data와 달리, 세상의 많은 요소들은 요소 사이의 순서와 arrangement rule이 고정되어 있지 않은 non-euclidean space에 존재한다. 이러한 non-euclidean space에서는, 기존의 traditional deep learning method들이 그대로 적용되기 어렵다. 단적인 예로, CNN의 convolution filter은, 특정 성분을 중심으로 n*n의 격자 kernel을 정의하지만, 상하좌우, 앞뒤 관계가 분명하지 않은 데이터, 가령 사람과 사람 사이의 네트워크 정보 등에서는, 같은 kernel을 정의하는 것이 불명확해진다.</p>

<p>이러한 non-euclidean space 상의 요소들은, ‘Graph Struture’을 정의함으로써 다룰 수 있다. Node 와 node를 연결하는 edge로 정의되는 그래프는 node 사이의 관계가 euclidian space에 한정되지 않고 자유도가 높다는 장점과 더불어, node feature과 edge weight으로 자유롭게 요소의 정보를 추가할 수 있다는 점에서 매력적이다.</p>

<p>하지만 데이터의 자유도가 큰만큼, graph를 분석하는 방법론 역시 다양하고 방대하다.</p>

<p>그래프를 공부하며, GNN, random walk, message passing, subgraph sampling, graph kernel, graph signal processing, graph structure learning, graph embedding, graph mining 등 끝없이 등장하는 용어들이 어느 층위에서 어떻게 다른 용어들과 연결되는 것인지 명확히 정리되지 않아 혼란함을 느꼈다. 나의 이해를 가시화해나가는 과정을 통해서 더 깊은 이해가 가능해지길, 또 글이 나와 비슷한 고민을 하는 누군가에게 조금이라도 도움이 되길 기대한다.</p>

<p>본 글에서는, graph representation의 framework를 개괄한다.
<br />
<br />
<br /></p>
<h1 id="graph-representation-learning이-무엇인가">Graph Representation Learning이 무엇인가</h1>
<hr />
<h2 id="-graph의-용어들"><span style="font-size:90%"> <em>Graph의 용어들</em></span></h2>
<p>Graph는 정보 unit(nodes)와, 사이의 연결관계(edge)로 이루어진, 나이브한 구조의 정보이다.</p>

<p>2차원으로 flattening된 이미지나, 해석 가능한 단어들의 조합으로 정렬된 언어와 달리, graph stucture의 high dimensional, non-euclidian information은 직관적인 해석이 불가하다. 따라서, 그래프에서 필요에 맞는 <strong>유용한 정보를 extract</strong>하는 방법은 오래전부터, 많은 분야에서 다루어져왔다.</p>
<p align="center">
  <img src="https://github.com/user-attachments/assets/42c6fd72-8baf-4029-8e2b-feec7395f4e4" width="600" />
</p>
<p align="center">  <em>Graph (출처: <a href="https://web.stanford.edu/class/cs224w/">CS224w)</a></em>
</p>

<p>일반적으로 그래프 $G$는 $G = (V, E)$의 tuple로 표현된다.</p>

<ul>
  <li>이때, $V = {v_1, v_2, \ldots, v_n}$ 는 $n$개의 노드 집합을 의미한다.</li>
  <li>$E = {e_1, e_2, \ldots, e_m} \subseteq V \times V$ 는 $m$개의 edge (노드와 노드를 연결하는 링크)를 의미한다.</li>
  <li>각 노드는 특성을 가질 수 있으며 <em>(예: 노드가 사람일 경우, 성별이라는 특성이 각 노드에 정의될 수 있다)</em> $k$ 종류의 특성이 있을 경우, 노드 특성 행렬 
\(\mathbf{X} \in \mathbb{R}^{k \times |V|}\)
로 표현된다.</li>
  <li>이렇게 정의된 그래프에서 edge의 양상은 인접행렬 (Adjacency Matrix) 
\(\mathbf{A} \in \mathbb{R}^{|V| \times |V|}\)
로 표현되며 만약 $(v_i, v_j) \in E$이면 $A_{ij} = 1$, 그렇지 않으면 $A_{ij} = 0$이다.</li>
</ul>

<h2 id="-learning의-목표는-그래프의-구조적-정보를-보존하는-것이다"><span style="font-size:90%"> <em>Learning의 목표는 그래프의 구조적 정보를 보존하는 것이다.</em></span></h2>

<p>그래프를 활용하기 위해서는, 그래프와 그 구성 요소들(예: 노드 및 엣지)을 다룰 수 있는 numerical features로 표현할 필요가 있다. 물론 위에 설명한 인접 행렬 $\mathbf{A}$ 자체도 그래프를 표현하는 방법이다. 그러나 인접 행렬은 크기가 $|V| \times |V|$이므로 매우 큰 그래프를 표현하기에는 소모적이며, 그래프의 중요한 정보를 보존하되 차원을 줄이는 feature extraction이 필요하다.</p>
<p align="center">
  <img src="https://github.com/user-attachments/assets/fe1e52bd-efee-41d4-aad0-e25074455606" width="800" />
</p>
<p align="center">  <em>Graph Representation Learning (출처: <a href="https://web.stanford.edu/class/cs224w/">CS224w)</a></em>
</p>
<ul>
  <li>고전적인 방법들은, 그래프에서 사전에 정의된 알고리즘으로 그래프에서 정보를 extract한다. Graph statistics(degrees, clustering coefficients), kernel functions 등이 이에 해당한다.  이러한 hand-engineered feature들은 산출 방법이 정해져 있는 ‘processing’이며, inflexible하다. 이러한 방법들은, 어느 정도 graph property를 반영하긴 하나, 오직 그래프를 정해진 방법으로 가공한 것이므로 feature의 깊이에 한계가 있다.</li>
  <li><strong>Graph Represenation Learning</strong>은, 이와 달리 그래프의 구조적 정보를 잘 보존하도록 embedding을 ‘learning’한다. ‘learning’한다는 것은, 설정된 ‘goal’에 가까워지게 최적화하는 것을 의미한다. Graph Representation Learning의 ‘goal’이 ‘그래프의 구조적 정보를 잘 보존하는 것’이 되는 셈이다.</li>
</ul>

<p align="center">
  <img src="https://github.com/user-attachments/assets/8c27c743-919c-4676-89a6-67c2055032f4" width="800" />
</p>
<p align="center"><em>Node Embedding</em></p>

<ul>
  <li>위 그림의 그래프 $G = (V, E)$를 예로 들어 살펴보자.
    <ul>
      <li>
        <p>그래프 구조에 대하서 설명하자면, 5개의 node($v_1, … , v_5$)와 6개의 연결된 edge로 이루어진 그래프는 왼쪽 아래의 인접행렬 $A$로 표현되며 붉게 표시된 $v_4$와 연결된 3개의 node ${v_2, v_3, v_5}$의 연결은 성분 1로 표현된다. 인접행렬의 대각성분은 자기 자신과의 연결은 정의하지 않으므로 0이며, (i,j) 연결과 (j,i) 연결은 undirected graph에서는 구분되지 않으므로 대칭행렬이다.</p>
      </li>
      <li>
        <p>$|V| = 5$인 이 그래프에서, 각 노드 $v_i$를 차원 4의 저차원 벡터로 인코딩하는 mapping function $f : v_i \rightarrow \mathbb{R}^4$ 
  을 학습하여, 그래프의 구조적 정보를 잘 보존하는 node embedding을 구하는 것이 바로 Graph Representation Learning인 것이다.</p>
      </li>
    </ul>
  </li>
</ul>

<blockquote>

  <p>그렇다면, learning의 goal인, <strong>그래프의 구조적 정보를 잘 보존함</strong>을 판단하는 기준은 무엇인가?</p>

</blockquote>

<h2 id="-구조적-정보는-유사도로-표현할-수-있다"><span style="font-size:90%"> <em>구조적 정보는 유사도로 표현할 수 있다.</em></span></h2>

<p>위 질문에 대한 답이, Graph Representation Learning의 핵심을 담는다. 직관적으로 생각할 때, ‘그래프의 구조적 정보’에 대한 gold label이 존재한다면 그 gold label에 가깝게 f를 learning하면 될 것이다. 하지만, 일반적으로 graph representaton learning 자체가 gold label이 없는 복잡한 자료구조에서 최대한 원래의 정보를 보존하는 representation을 구해내는 것에 의의를 두기에, 일반적으로 learning은 외부에서 주어지는 정답지가 없는, unsupervised learning의 형태로 이루어진다.</p>

<p>Learing의 핵심적인 아이디어는, 아래와 같다.</p>
<blockquote>

  <p>Embedding에서 node 간의 <strong><em>유사도(proximity)</em></strong> 가 원래 그래프와 비슷하게 보존된다면, 간접적으로 <strong><em>그래프의 중요한 구조 정보가 보존</em></strong>된다고 볼 수 있다.</p>

</blockquote>
<p align="center">
  <img src="https://github.com/user-attachments/assets/42cfae28-12ef-47d4-bfe0-d43ac91a608b" width="800" />
</p>
<p align="center"><em>유사도가 '거리'로 정의되는 상황</em></p>

<p>예를 들어보겠다. 지구에 살고 있는 영이($v_1$), 영희($v_2$), Mike($v_3$)를, 극좌표계의 단위벡터로 표현한다고 생각해보자. 훌륭한 representation이 되기 위해서는, <strong>사람들 간의 유사도가, 극좌표계 상에서도 보존되어야 한다.</strong> 이 task를 단순명료화하기 위해서 아래와 같은 두 전제를 할 수 있다.</p>

<ol>
  <li><strong>전제 1</strong> - ‘사람들 사이의 유사도’는 사람들 간의 ‘거주하는 지역 간의 거리’가 가까울수록 크다.</li>
  <li><strong>전제 2</strong> - 극좌표계에서 ‘벡터들 간의 유사도’는 벡터 사이의 각도가 작을수록 크다.</li>
</ol>

<p>위 두 전제 하에 지구에서 영이와 영희가 옆집에 살고, Mike는 혼자 먼 섬나라에 산다는 것을 반영하면, 극좌표계 상에서 표현된 (영이, 영희) 사이의 각도가 좁고, (영희, Mike), (영이, Mike)의 각도가 큼은 합리적이다.
즉 지구에서 사람들 간의 ‘거주하는 지역 간의 거리’ 정보가, 극좌표계 상의 벡터 간 사잇각 정보로 reconstruct되었다고 볼 수 있다. 이는 ‘거리 정보’가 보존된다는 점에서 훌륭한 representation이다.</p>

<p>하지만, 위의 두 전제에 관해 드는 의문이 있다.</p>
<ol>
  <li><strong>전제 1 - Node 간 유사도에 대한 의문</strong> : 사람은 셀 수 없이 많은 각기 다른 특성들을 갖고 있다. 머리모양에서부터 목소리, 옷, 성별, 나이, 인종… 이 많은 특성들을 깡그리 무시하고 <strong>‘거주하는 지역 간의 거리’가 ‘사람들 사이의 유사도’를 나타낸다고 가정하는 것이 합리적인가?</strong></li>
  <li><strong>전제 2 - embedding space에서 보존되는 유사도 대한 의문</strong> : 비슷한 맥락으로, <strong>극좌표계의 두 임베딩 벡터 $z_i, z_j$의 유사도를 사잇각으로 정의하는 것이 합리적인가?</strong></li>
</ol>

<p><br />
<br /></p>

<h1 id="learning-framework">Learning Framework</h1>
<hr />
<p>결론부터 이야기하면, 위의 두 전제, 즉 Representation Learning에서 <strong>유사도의 정의</strong>는 연구자의 의도, 혹은 downstream task에 따라 달라진다. 따라서 representation 학습은 본질적으로, <strong>어떤 그래프 상의 유사도를 어떤 방식으로 보존할 것인가</strong>에 대한 설계와 그 구현을 포함하는 과정이다.</p>

<p align="center">
  <img src="https://github.com/user-attachments/assets/dabdcbcd-1a20-48f9-8233-25e5a208a8ba" width="800" />
</p>
<p align="center"><em>Learning Scheme</em></p>

<p>위에서 다룬 그래프 $G = (V, E)$를 다시 갖고 와, 지금까지 설명한 내용을 수식으로 살펴보자.</p>

<h2 id="-1유사도의-정의"><span style="font-size:90%"> <em>1.유사도의 정의</em></span></h2>
<blockquote>
  <p>연구자는 두 가지 유사도를 정의한다. 하나는 <strong>그래프 상의 유사도</strong>이며, 다른 하나는 <strong>embedding space 상의 유사도</strong>이다.</p>
</blockquote>

<ul>
  <li>
    <p><strong>Pairwise Similarity Function(노랑)</strong></p>

    <p><strong>그래프 $G = (V, E)$ 위에서 정의되는, 노드 간의 유사도를 나타내는 함수이며 아래와 같이 표현된다.</strong></p>

\[s_G : V \times V \rightarrow \mathbb{R}^+\]

    <p>이 함수는 노드 $v_i$와 $v_j$ 사이의 <strong>그래프 기반 유사도</strong>를 정의한다. 
예를 들어,</p>

    <ul>
      <li>$s_G(v_i, v_j) = A_{ij}$일 경우, 유사도 자체는  노드 간 인접 여부 (adjacency matrix)를 의미한다.</li>
      <li>Random walk 기반 representation learning에서 $s_G(v_i, v_j)$는 random walk에서의 공출현 확률로 정의될 수 있다.</li>
    </ul>
  </li>
  <li>
    <p><strong>Decoder Function(초록)</strong></p>

    <p><strong>두 임베딩 벡터 $z_i, z_j$의 유사도를 계산하는 함수이며 아래와 같이 표현된다.</strong></p>

\[\text{DEC} : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}^+\]

    <p>일반적으로 학습 parameter를 가지지 않으며 단순한  형태이다. 예를 들어,</p>

    <ul>
      <li>$\text{DEC}(z_i, z_j) = z_i^\top z_j$일 경우, 두 벡터 간 inner product를 의미한다.</li>
      <li>$\text{DEC}(z_i, z_j) = -|z_i - z_j|^2$일 경우, 두 벡터 간 euclidean distance를 의미한다.</li>
    </ul>
  </li>
</ul>

<h2 id="-2learning의-과정"><span style="font-size:90%"> <em>2.Learning의 과정</em></span></h2>

<blockquote>
  <p><strong>최적화되는 대상은 Encoder function</strong>이며, <strong>학습의 목적은 Loss Function을 최소화하는 것</strong>이다.</p>
</blockquote>

<ul>
  <li>
    <p><strong>Encoder Function</strong></p>

    <p><strong>그래프 상의 노드 $v_i \in V$를 $d$차원 벡터로 임베딩하는 함수이며 아래와 같이 표현된다.</strong></p>

\[\text{ENC} : V \rightarrow \mathbb{R}^d\]

    <p>즉, 노드 $v_i$는 임베딩 벡터 $z_i = \text{ENC}(v_i)$로 매핑된다. 이 함수는 학습 가능한 파라미터를 가지며, 학습 과정에서 최적화된다.</p>
  </li>
  <li>
    <p><strong>Loss Function</strong></p>

    <p><strong>복원된 유사도 $\text{DEC}(z_i, z_j)$와 원래 유사도 $s_G(v_i, v_j)$ 간의 차이를 최소화하는 손실 함수이다.</strong></p>

    <p>손실함수 \(\ell : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}\) 는 복원된 유사도와 실제 유사도 간의 차이를 측정하며, 전체 학습 손실은 다음과 같이 정의된다.</p>

\[\mathcal{L} = \sum_{(v_i, v_j) \in \mathcal{D}} \ell\left(\text{DEC}(z_i, z_j), s_G(v_i, v_j)\right)\]

    <p>여기서 $\mathcal{D}$는 학습에 사용되는 노드 쌍들의 집합이다.</p>
  </li>
</ul>

<p><br />
<br /></p>

<hr />

<h1 id="정리하면">정리하면</h1>

<p>위 프레임워크를 공통적인 틀로 가지는 Graph Representation Learning은 결국 아래의 두 요소가 본질이다.</p>

<blockquote>

  <ol>
    <li><strong>유사도 설정 : $s_G$(그래프 상의 유사도)와 Decoder(Decoding한 유사도)를 어떻게 정의하는가?</strong></li>
    <li><strong>Learning의 과정 : 어떤 구조의 encoder를 사용하는가?, loss 함수는 어떻게 정의하는가?</strong></li>
  </ol>

</blockquote>

<p>위 요소의 차이에 따라 Graph Representation Learning의 Method의 세부 분류가 나누어진다.</p>

<p>사실, 스탠포드 CS224w의 강의자이시자 위 프레임워크를 제안하신 Jure Leskovec 교수님께서 지적하셨듯, GRL 방법들은 다양한 벤치마크와 모델 개념들이 산발적으로 존재하여 이론적 통일성이 결여되어 있다. 각기 다른 하위 분야에서 독립적으로 연구되어 온 GRL의 task 중심 평가에서 벗어나, 본질적으로 우리가 어떤 graph 구조를 representation에 담고자 하는지, 어떻게 그것을 encoding해야 하는지, latent space에 어떤 제약을 둘 것인지에 대한 체계화된 개념론적 논의가 더 활발히 이루어지길 소망한다.</p>

<hr />

<p><em>참고문헌</em></p>
<ol>
  <li>Khoshraftar, Shima, and Aijun An. “A survey on graph representation learning methods.” ACM Transactions on Intelligent Systems and Technology 15.1 (2024): 1-55.</li>
  <li>Ju, Wei, et al. “A comprehensive survey on deep graph representation learning.” Neural Networks 173 (2024): 106207.</li>
  <li>Hamilton, William L., Rex Ying, and Jure Leskovec. “Representation learning on graphs: Methods and applications.” arXiv preprint arXiv:1709.05584 (2017).</li>
</ol>]]></content><author><name>DaRe_jin</name></author><category term="[&quot;Graphs&quot;]" /><category term="Graphs" /><category term="Representation learning" /><summary type="html"><![CDATA[]]></summary></entry></feed>